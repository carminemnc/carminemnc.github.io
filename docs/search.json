[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "CausalwizMay 7, 2025\n\n\n\nTools for Causal Inference Analysis and Visualization\n\n\n\n\n\n\nBook your spaceNov 23, 2022\n\n\n\nAn open source application for managing office desk booking\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/book-your-space/index.html",
    "href": "projects/book-your-space/index.html",
    "title": "Book your space",
    "section": "",
    "text": "Demo  Source code"
  },
  {
    "objectID": "projects/book-your-space/index.html#motivation",
    "href": "projects/book-your-space/index.html#motivation",
    "title": "Book your space",
    "section": "Motivation",
    "text": "Motivation\nThe COVID-19 pandemic breakout has led to a significant shift in the way we work, with many employees now working remotely or in a hybrid model. This has created a need for new tools and technologies to help businesses manage their office space more effectively.\nThis application for managing desk office booking is designed to address this challenge."
  },
  {
    "objectID": "projects/book-your-space/index.html#how-it-works",
    "href": "projects/book-your-space/index.html#how-it-works",
    "title": "Book your space",
    "section": "How it works",
    "text": "How it works\nBuild on Google apps script platform, the application is designed to work in a Google enviroment.\nIt allows:\n\nPerson quota managing over 2 weeks of booking\nDisplay names customization\nBookings download\n\nFork the script here and adapt it on your needs."
  },
  {
    "objectID": "notebooks/eda/index.html",
    "href": "notebooks/eda/index.html",
    "title": "How to get your loan",
    "section": "",
    "text": "Dataset"
  },
  {
    "objectID": "notebooks/eda/index.html#motivation-and-analysis-design",
    "href": "notebooks/eda/index.html#motivation-and-analysis-design",
    "title": "How to get your loan",
    "section": "Motivation and analysis design",
    "text": "Motivation and analysis design\nThis analysis is intended to serve as an exercise in data visualisation and exploratory data analysis (EDA) techniques. The dataset comprises 20.000 synthetic records of personal and financial information. These records include various features such as demographic information, credit history, employment status, income levels, existing debt, and other relevant financial metrics. This provides a comprehensive basis for sophisticated, data-driven analysis and decision-making. The objective of this exercise is to analyse the dataset in order to identify patterns, relationships, and the factors influencing loan decisions.\nWhile the insights gained may be useful, it should be noted that the data is synthetic, and the primary focus is on demonstrating various visualisation methods and developing those used in the datasage python package.\n\n\n\n\n\n%%{init: {'theme': 'default', 'themeVariables': { 'fontFamily': 'Arial, sans-serif', 'fontSize': '20px', 'primaryColor': '#0085a1', 'primaryTextColor': 'black' }}}%%\ngraph TD\n    A((Loan&lt;br&gt;Approval)) --- B((Income&lt;br&gt;&nbsp;))\n    A --- C((Who&lt;br&gt;gets&lt;br&gt;loans?))\n    A --- E((Credit&lt;br&gt;History))\n    A --- F((Wealth))\n    \n    classDef main fill:#0085a1\n    classDef factor fill:#0085a1\n    \n    class A main\n    class B,C,D,E,F factor"
  },
  {
    "objectID": "notebooks/eda/index.html#loan-approval-landscape",
    "href": "notebooks/eda/index.html#loan-approval-landscape",
    "title": "How to get your loan",
    "section": "Loan approval landscape",
    "text": "Loan approval landscape\n\n\nCode\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 4))\n\n# binary ratio plot\nleo.binary_ratio_plot(data=df, column_name='loan_approved', target_zero_name=\"Not approved\", target_one_name=\"Approved\", ax=ax1)\n\n# let's see distribution of risk score by approval status\nsns.histplot(data=df, x='risk_score', hue='loan_approved', multiple=\"layer\", alpha=0.6, ax=ax2)\nax2.set_xlabel('Risk score')                     \n \nax2.set_ylabel('Frequency')\nax2.legend(labels=['Approved', 'Not approved'],loc='upper right')\n\nleo.insights_box(fig,\n                 fontsize=12,\n                 position='bottom',\n                 x=0,\n                 y=-0.02,\n                 text=\n                 \"\"\"\n                 As expected data is heavily unbalanced revealing a disparity in loan approvals. \n                 Only 23.90% are accepted, versus 76.10% rejections.\n                 \n                 Approved loans predominantly cluster between 30 and 45 points, peaking around 40.\n                 In contrast, rejected loans dominate beyond 50 points, with the highest frequency between 50 and 60.\n                 This marked separation underscores how a low risk score is crucial for loan approval, \n                 suggesting that 45-50 points might represent the critical threshold for acceptance.\n                 \"\"\")                                                                                                                                                                                                   \nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/eda/index.html#income-first-line-of-defense",
    "href": "notebooks/eda/index.html#income-first-line-of-defense",
    "title": "How to get your loan",
    "section": "Income: first line of defense",
    "text": "Income: first line of defense\n\n\nCode\nfig, (ax1, ax2, ax3) = leo.create_layout( [(0,0,1,1), (0,1,1,1), (1,0,1,2)], figsize=(10,6))\n\n# let's see the annual income of who get the loan approved\nsns.kdeplot(data=df, x='annual_income', hue='loan_approved', fill=True, alpha=0.5, ax=ax1, legend=False)\nax1.set_title('Annual income by approval status')\nax1.set_xlabel('Annual income')\nax1.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:,.0f}K'))\nax1.legend(labels=['Approved', 'Not approved'], loc='upper right')\n\n# how the income affect the loan amount requested?\nsns.scatterplot(data=df, x='annual_income', y='loan_amount', hue='loan_approved',sizes=(20, 200), alpha=0.6, ax=ax2)\nax2.set_title('Annual income vs Loan amount')\nax2.set_xlabel('Annual income')\nax2.set_ylabel('Loan amount')\nax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:,.0f}K'))\nax2.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:,.0f}K'))\nhandles, _ = ax2.get_legend_handles_labels()\nax2.legend(handles=handles, labels=['Not approved', 'Approved'], loc='upper right')\n\n# how monthly income change with different risk score levels\nsns.violinplot(data=df, x='risk_level', y='monthly_income', ax=ax3, legend=False)\nsns.swarmplot(data=df.head(1000), x='risk_level', y='monthly_income', color='black', size=2, ax=ax3, legend=False)\nax3.set_ylabel('Monthly income')\nax3.set_xlabel('Risk score')\nax3.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:,.0f}K'))\n\nleo.insights_box(fig,\n                 fontsize=12,\n                 position='top',\n                 x=0,\n                 y=1.43,\n                 text=\n                 \"\"\"\n                 Most applications come from individuals earning $0-\\$100K, increasing for for incomes above \\$100K.\n                 This suggests that a higher income does not necessarily guarantee loan approval.\n                 \n                 Approved loans are mainly concentrated in the $100K-\\$300K income range, with amounts rarely exceeding \\$50K. \n                 Interestingly, for higher incomes (&gt;\\$300K), approved loan amounts tend to remain conservative. \n                 Regarding rejected loans (blue dots), they show greater variability in requested amounts, \n                 with some applications reaching \\$175K, especially in lower income brackets (&lt;\\$100K).\n                 This suggests that loan requests disproportionate to income are more likely to be rejected.\n\n                 The income distribution widens towards lower brackets as risk score increases, \n                 indicating an inverse correlation between income and risk.\n                 \"\"\")  \n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/eda/index.html#who-gets-loans",
    "href": "notebooks/eda/index.html#who-gets-loans",
    "title": "How to get your loan",
    "section": "Who gets loans?",
    "text": "Who gets loans?\nThe analysis of demographic variables reveals a clear hierarchy of factors influencing loan approval. Education emerges as the most decisive factor, with a significant gap between approval rates for higher degrees (PhD 48-63%) and lower ones (High School 16-22%). This pattern is further reinforced by age: approval rates consistently increase with age across all education levels, suggesting that life experience and financial stability are highly valued. Professional stability also plays a key role, with self-employed and employed individuals enjoying significantly higher approval rates than the unemployed. Surprisingly, marital status has a marginal impact, suggesting a more equitable approach based on individual merit rather than social factors.\n\n\nCode\napproval_matrix = pd.pivot_table(df, values='loan_approved',index='education_level',columns='age_bins',aggfunc='mean')\n\nplt.figure(figsize=(10, 5))\nsns.heatmap(approval_matrix, annot=True, fmt='.0%',cmap=ccmap,cbar=False)\nplt.title('Loan approval rate by age and education level')\nplt.xlabel('Age Group')\nplt.ylabel('')\nplt.tight_layout()\nplt.show()\n\nfig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2, 2, figsize=(10, 7))\n\nleo.stacked_bars_plot(data=df, x='employment_status', y='loan_approved', ax=ax1)\nax1.set_title('Loan approval status by employment status')\nleo.stacked_bars_plot(data=df, x='marital_status', y='loan_approved', ax=ax2)\nax2.set_title('Loan approval status by marital status')\nleo.stacked_bars_plot(data=df, x='job_tenure_bins', y='loan_approved', ax=ax3)\nax3.set_title('Loan approval status by job tenure')\nleo.stacked_bars_plot(data=df, x='experience_bins', y='loan_approved', ax=ax4)\nax4.set_title('Loan approval status by experience')\n\nax2.legend(title='Approval status', labels=['Not approved', 'Approved'], bbox_to_anchor=(1, 1), loc='upper left')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/eda/index.html#wealth-indicators",
    "href": "notebooks/eda/index.html#wealth-indicators",
    "title": "How to get your loan",
    "section": "Wealth indicators",
    "text": "Wealth indicators\n\n\nCode\nfig, (ax1, ax2, ax3) = leo.create_layout( [(0,0,1,1), (0,1,1,1), (1,0,1,2)], figsize=(10, 7))\n\n# risk score and loan approval status by net worth level\nleo.dumbbell_plot(df=df, group_col='loan_approved',category_col='net_worth_level',value_col='risk_score',ax=ax1,labels=['Not approved','Approved'])\nax1.set_title('Risk score comparison by net worth level')\nax1.set_xlabel('Risk score')\n\n# total assets and liabilities proportions by net worth level\ndf.groupby('net_worth_level')[['asset_percentage', 'liability_percentage']].mean().rename(columns={'asset_percentage': 'Assets', 'liability_percentage': 'Liabilities'}).plot(kind='area', stacked=True, ax=ax2)\nax2.set_title('Assets and liabilities composition in net worth')\nax2.set_xlabel('Net worth level')\nax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\nax2.set_xticks(range(5))\nax2.set_xticklabels(['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n\n# the asset coverage (total assets / loan amount) ratio by risk level\nax = sns.boxplot(data=df, x='risk_level', y='asset_coverage_ratio', showfliers=False, hue='loan_approved', width=0.3, ax=ax3)\nax3.set_title('Asset coverage ratio by risk level')\nax3.set_xlabel('Risk level')\nax3.set_ylabel('Asset coverage ratio')\nhandles, _ = ax3.get_legend_handles_labels()\nax3.legend(handles=handles, labels=['Not approved', 'Approved'])\n\nleo.insights_box(fig,\n                 fontsize=12,\n                 position='bottom',\n                 x=0,\n                 y=-0.02,\n                 text=\n                 \"\"\"\n                 The risk score shows a clear correlation with net worth.\n                 'Very Low' and 'Low' levels present lower risk scores (37.5-45). 'High' and 'Very High' levels show higher scores (47.5-55).\n                 \n                 Net worth composition reveals an interesting trend.\n                 As net worth increases, the proportion of assets to liabilities significantly improves.\n\n                 The asset coverage ratio plot reveals that for 'Low' risk level, we observe the widest variability (1-20x) \n                 and highest medians (around 5x) for both approved and non-approved loans, \n                 suggesting that strong asset coverage doesn't automatically guarantee approval.\n                 \"\"\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/eda/index.html#credit-history-trust-factor",
    "href": "notebooks/eda/index.html#credit-history-trust-factor",
    "title": "How to get your loan",
    "section": "Credit history: trust factor",
    "text": "Credit history: trust factor\nBankruptcy history plays a decisive role: the majority of applicants have no bankruptcy history and show risk scores concentrated in the 50-70 range, while those with bankruptcy records display a more scattered and generally riskier distribution. The ridge plot of credit history length reveals a particularly interesting pattern in risk distribution. Low-risk profiles show a distinctive peak around 30 years of credit history, suggesting that lengthy credit experience is associated with lower risk. In contrast, high-risk profiles display a pronounced peak around 15 years, with the distribution tapering off towards longer credit histories. This divergence in peaks between high and low-risk profiles suggests that credit history longevity can be a significant indicator of applicant reliability\n\n\nCode\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n\nsns.scatterplot(data=df, x='risk_score', y='credit_score',hue='loan_approved', ax=ax1, alpha=0.5)\nax1.set_title('Credit score vs Risk score')\nax1.set_xlabel('Risk score')\nhandles, _ = ax1.get_legend_handles_labels()\nax1.legend(handles=handles, labels=['Not approved', 'Approved'], loc='upper right')\n\nsns.kdeplot(data=df, x='risk_score', hue='bankruptcy_history', fill=True, alpha=0.5, ax=ax2, legend=False)\nax2.set_title('Risk score by Bankruptcy history')\nax2.set_xlabel('Risk score')\nax2.legend(labels=['With history', 'Without history'], loc='upper right')\n\nplt.tight_layout()\nplt.show()\n\nleo.ridge_plot(data=df,x_var='length_of_credit_history',group_var='risk_level',cmap=ccmap,height=1,aspect=9,fontsize_facets=8)\nplt.xlabel('Length of credit history')\nplt.suptitle('Length of credit history by risk level',y=0.90,size=10,va='baseline')\nplt.show()"
  },
  {
    "objectID": "notebooks/eda/index.html#the-approval-formula",
    "href": "notebooks/eda/index.html#the-approval-formula",
    "title": "How to get your loan",
    "section": "The approval formula",
    "text": "The approval formula\nAs highlighted in our previous analyses, we can now precisely quantify the impact of each variable on the risk score through their correlations.\n\nOn one side, we find factors that increase risk: bankruptcy history emerges as the most critical element, with a positive correlation of 0.4, followed by total debt-to-income ratio and debt-to-income ratio (0.35). Interest rates and previous defaults also contribute to increasing risk, albeit with a more moderate impact (0.2).\nOn the other side, some factors act as protective elements, reducing risk: income, both monthly and annual, shows the strongest negative correlation (-0.4), followed by net worth and total assets (-0.3). Credit score and length of credit history confirm their protective role with moderate negative correlations (-0.25). It’s particularly interesting to note how the requested loan amount has a relatively modest impact on the risk score (0.15), suggesting that banks focus more on repayment capability than on the loan size itself.\n\n\n\nCode\n# Select numeric columns only\nnumeric_cols = cldf.select_dtypes(include=['int64', 'float64']).columns.tolist()\nnumeric_cols.remove('risk_score')\n\n# Calculate correlations with risk_score\ncorrelations = cldf[numeric_cols + ['risk_score']].corr()['risk_score'].drop(['risk_score','loan_approved']).sort_values(key=abs, ascending=False)\n\n# renaming index\ncorrelations.index = [col.replace('_', ' ').capitalize() for col in correlations.index]\n\nplt.figure(figsize=(10, 4))\nbars = correlations.head(15).plot(kind='barh', color=np.where(correlations &gt; 0, '#0085a1', '#242728'),alpha=1)\nplt.axvline(x=0, color='#242728', linestyle='--', alpha=0.1)\nplt.xlabel('Correlation')\nplt.title('Correlation with risk score')\nmplcyberpunk.add_glow_effects()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "I’m a Statistician with a specialization in actuarial science.\nThis space contains projects, notebooks with raw notes about data science, and posts where I explore mathematical and statistical topics in depth. I explore different fields with statistical rigor, aiming to bridge theory and practical applications."
  },
  {
    "objectID": "notebooks/apriori-defects/index.html",
    "href": "notebooks/apriori-defects/index.html",
    "title": "Apriori analysis for quality assurance",
    "section": "",
    "text": "The Apriori 1 algorithm is a foundational data mining technique designed to discover frequent patterns and associations within datasets. Originally developed for market basket analysis — identifying which products customers commonly purchase together — it has since proven valuable across diverse domains including healthcare and web usage analysis.\nThe algorithm operates on a simple principle: it systematically examines data to identify which items frequently appear together, building from individual items to larger combinations based on their occurrence patterns.\n\n\\begin{aligned}\n\\begin{array}{c|c}\n\\hline\n\\text { Case } & \\text { Defects } \\\\\n\\hline\nCase \\ 1 & [Defect \\ 1, Defect \\ 2,Defect \\ 3] \\\\\nCase \\ 2 & [Defect \\ 1, Defect \\ 2] \\\\\nCase \\ 3 & [Defect \\ 1, Defect \\ 4] \\\\\nCase \\ 4 & [Defect \\ 1, Defect \\ 4,Defect \\ 5,Defect \\ 6] \\\\\n\\end{array}\n\\end{aligned}\n\n\n\nIn quality assurance, the Apriori algorithm transforms how we analyze defect patterns. Rather than treating each defect as an isolated incident, it reveals the interconnected nature of quality issues across process instances. For instance, discovering that interface errors and data validation failures frequently co-occur suggests these aren’t isolated problems but symptoms of a deeper integration issue.\nThe practical applications span multiple quality management areas:\n\nStrategic Process Improvement: Instead of addressing defects randomly, teams can prioritize areas where multiple quality issues cluster together, maximizing the impact of improvement efforts.\nEnhanced Root Cause Analysis: When certain defects consistently appear together, it signals shared underlying causes. This pattern recognition accelerates problem-solving by directing investigation toward common sources rather than treating symptoms separately.\nOperational Efficiency: Understanding defect relationships allows teams to design remediation strategies that address multiple issues simultaneously, reducing redundant work and accelerating resolution times.\nPredictive Quality Management: Once patterns are established, the presence of one defect type can serve as an early warning system for related issues, enabling proactive intervention before problems cascade."
  },
  {
    "objectID": "notebooks/apriori-defects/index.html#the-apriori-algorithm",
    "href": "notebooks/apriori-defects/index.html#the-apriori-algorithm",
    "title": "Apriori analysis for quality assurance",
    "section": "",
    "text": "The Apriori 1 algorithm is a foundational data mining technique designed to discover frequent patterns and associations within datasets. Originally developed for market basket analysis — identifying which products customers commonly purchase together — it has since proven valuable across diverse domains including healthcare and web usage analysis.\nThe algorithm operates on a simple principle: it systematically examines data to identify which items frequently appear together, building from individual items to larger combinations based on their occurrence patterns.\n\n\\begin{aligned}\n\\begin{array}{c|c}\n\\hline\n\\text { Case } & \\text { Defects } \\\\\n\\hline\nCase \\ 1 & [Defect \\ 1, Defect \\ 2,Defect \\ 3] \\\\\nCase \\ 2 & [Defect \\ 1, Defect \\ 2] \\\\\nCase \\ 3 & [Defect \\ 1, Defect \\ 4] \\\\\nCase \\ 4 & [Defect \\ 1, Defect \\ 4,Defect \\ 5,Defect \\ 6] \\\\\n\\end{array}\n\\end{aligned}\n\n\n\nIn quality assurance, the Apriori algorithm transforms how we analyze defect patterns. Rather than treating each defect as an isolated incident, it reveals the interconnected nature of quality issues across process instances. For instance, discovering that interface errors and data validation failures frequently co-occur suggests these aren’t isolated problems but symptoms of a deeper integration issue.\nThe practical applications span multiple quality management areas:\n\nStrategic Process Improvement: Instead of addressing defects randomly, teams can prioritize areas where multiple quality issues cluster together, maximizing the impact of improvement efforts.\nEnhanced Root Cause Analysis: When certain defects consistently appear together, it signals shared underlying causes. This pattern recognition accelerates problem-solving by directing investigation toward common sources rather than treating symptoms separately.\nOperational Efficiency: Understanding defect relationships allows teams to design remediation strategies that address multiple issues simultaneously, reducing redundant work and accelerating resolution times.\nPredictive Quality Management: Once patterns are established, the presence of one defect type can serve as an early warning system for related issues, enabling proactive intervention before problems cascade."
  },
  {
    "objectID": "notebooks/apriori-defects/index.html#about-the-data",
    "href": "notebooks/apriori-defects/index.html#about-the-data",
    "title": "Apriori analysis for quality assurance",
    "section": "About the data",
    "text": "About the data\nThis dataset represents an anonymized subset of real-world quality assurance data, providing an authentic foundation for demonstrating the Apriori algorithm’s practical applications. While the specific defect identifiers have been obscured, the underlying patterns and relationships remain intact, offering genuine insights into how defect associations manifest in actual production environments."
  },
  {
    "objectID": "notebooks/apriori-defects/index.html#the-three-core-metrics",
    "href": "notebooks/apriori-defects/index.html#the-three-core-metrics",
    "title": "Apriori analysis for quality assurance",
    "section": "The three core metrics",
    "text": "The three core metrics\nThe Apriori algorithm identifies meaningful relationships through three key metrics that work together to reveal patterns:\n\nSupport measures how frequently a item combination appears across all items. It answers: “How common is this pattern?” High support indicates widespread occurrence. For example, within quality management areas, this could help prioritise the most prevalent issues.\nConfidence quantifies the reliability of a relationship by measuring how often item Y occurs when item X is present. It answers: “If I see item X, how likely am I to also find item Y?”\nLift determines whether items occur together more often than random chance would suggest.\n\nThese metrics work in combination to filter noise from meaningful patterns. For instance, a rule with high support ensures the pattern is frequent enough to matter, high confidence makes it reliable for prediction, and high lift confirms the relationship isn’t just coincidental.\n\nSupport\nMathematically defined as the proportion of transactions containing a specific itemset, support answers the fundamental question: “How often does this pattern appear?”\n\\text{support}({X}) = \\frac{\\text{Number of transactions containing } X}{\\text{Total number of transactions}}\nIn quality assurance contexts, high support values indicate widespread defect patterns that warrant immediate attention due to their prevalence. For instance, if a defect combination has 20% support, it means this pattern occurs in one out of every five cases, suggesting a systematic issue requiring priority investigation. Support serves as the foundation for identifying frequent patterns before analyzing their relationships through confidence and lift metrics.\n\n\nConfidence\nThe confidence of a rule {X} -&gt; {Y} is defined as\n\\text{confidence}({X} \\rightarrow {Y}) = P({Y} | {X})\nwhere P({Y} | {X}) is the conditional probability of observing Y given that X has occurred. Mathematically, this is expressed as: P({Y} | {X}) = \\frac{P(X \\cap Y)}{P(X)}\nwhere P(X \\cap Y) represents the joint probability of both events occurring together, and P(X) is the marginal probability of event X. The confidence metric measures how reliable or trustworthy the association rule {X} -&gt; {Y} is. It represents the proportion of transactions containing X that also contain Y. In other words, it answers the question: “When X occurs, how often does Y also occur?”\nA confidence value closer to 1 (or 100%) indicates a stronger association between X and Y, meaning that if X is present in a transaction, we can be more confident that Y will also be present. Conversely, a confidence value closer to 0 suggests a weaker association, implying that the presence of X does not reliably indicate the presence of Y.\n\nConfidence: interpretation\nLet’s examine rule #3 where:\nX = D_{18}\nY = D_7\nWith \\text{confidence}({X} \\rightarrow {Y}) = 0.75, we can draw the following conclusions:\n\nPredictive Reliability: The 75% confidence indicates that when defect D_{18} occurs, there’s a three-in-four chance that defect D_7 will also be present. This makes D_{18} a reliable predictor for D_7.\nShared Root Cause: Such a strong association suggests these defects likely stem from a common underlying issue rather than independent failures. This points to systemic problems in related process components.\n\n\n\n\nLift\nThe lift of a rule {X} -&gt; {Y} is defined as\n\\text{lift}({X} \\rightarrow {Y}) = \\frac{P({Y} | {X})}{P({Y})}\nwhere P({Y} | {X}) is the conditional probability of observing Y given that X has occurred, and P({Y}) is the marginal probability of observing Y. A lift value greater than 1 indicates that the co-occurrence of X and Y is higher than expected if they were statistically independent, while a lift value less than 1 indicates that the co-occurrence is lower than expected.\n\nLift: interpretation\nContinuing with rule #3 where:\nX = D_{18}\nY = D_7\nThe probability of defect D_7 occurring, without considering any other factors, is 5%. In other words, in 5% of all cases, defect D_7 is present.\nHowever, when you look at cases where defect D_{18} is present, you find that the probability of defect D_7 occurring is ~41.9%.\nUsing the lift formula, we can calculate the lift of the rule D_{18} → D_7 as follows:\n\\text{lift}({X} \\rightarrow {Y}) = \\frac{P({Y} | {X})}{P({Y})} = \\frac{0.419}{0.05} = 8.39\nThe lift value of 8.39 indicates that when defect D_{18} is present, the probability of defect D_7 occurring is 8.39 times higher than the probability of defect D_7 occurring without considering the presence of D_{18}.\nThis suggests that the occurrence of D_{18} and D_7 together is not independent or random, but rather there is a strong positive association between the two defects. This information can be used to further investigate the relationship between D_{18} and D_7 and potentially uncover underlying factors or dependencies that contribute to their co-occurrence."
  },
  {
    "objectID": "notebooks/apriori-defects/index.html#an-example-in-r",
    "href": "notebooks/apriori-defects/index.html#an-example-in-r",
    "title": "Apriori analysis for quality assurance",
    "section": "An example in R",
    "text": "An example in R\nThe following implementation demonstrates how to apply Apriori analysis to defect data using R. The interactive table below shows the discovered association rules, with color-coded metrics to highlight the strongest patterns. The network visualization reveals the relationships between defects.\n\nrules &lt;- apriori(trans,\n1                 parameter = list(supp=3/length(items_list),\n2                                  conf=0.1,\n                                  target= \"rules\"),\n                 control = list(verbose=FALSE))\n\n\n1\n\n3/length(items_list) means minimum 3 out of total transactions must contain an itemset for it to be considered frequent\n\n2\n\nconf=0.1 means the minimum confidence threshold is set to 0.1 or 10%. This filters out association rules where the consequent (right-hand side) occurs less than 10% of the times when the antecedent (left-hand side) occurs\n\n\n\n\n\n\n\n\n\n\n\n\nplot(rules,\n     method = \"graph\",\n     engine = \"htmlwidget\",\n1     shading = \"confidence\")\n\n\n1\n\nNodes with higher confidence rules will be shaded darker, allowing you to visually identify the most confident association rules at a glance"
  },
  {
    "objectID": "notebooks/apriori-defects/index.html#footnotes",
    "href": "notebooks/apriori-defects/index.html#footnotes",
    "title": "Apriori analysis for quality assurance",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nApriori wikipedia page↩︎"
  },
  {
    "objectID": "notebooks.html",
    "href": "notebooks.html",
    "title": "Notebooks",
    "section": "",
    "text": "How to get your loanJun 1, 2025\n\n\n\nA data-driven journey through the factors that make or break loan applications\n\n\n\n\n\n\nApriori analysis for quality assuranceAug 9, 2024\n\n\n\nPriori Patterns for Posterior Perfection\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "On DAG basicsFeb 26, 2026\n\n\n\nExperimental design through causal thinking\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/causalwiz/index.html",
    "href": "projects/causalwiz/index.html",
    "title": "Causalwiz",
    "section": "",
    "text": "Package doc  Source code\ncausalwiz is an R package for causal inference analysis. It provides functions for estimating treatment effects using various statistical methods, features built-in plotting functions an utilities designed for causal inference workflows in R."
  },
  {
    "objectID": "projects/causalwiz/index.html#installation",
    "href": "projects/causalwiz/index.html#installation",
    "title": "Causalwiz",
    "section": "Installation",
    "text": "Installation\nYou can install the package via Github repository:\n# install.packages(\"pak\")\npak::pak(\"carminemnc/causalwiz\")"
  }
]