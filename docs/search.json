[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "",
    "section": "",
    "text": "Tripscraper\n\n\nAn open source application for NLP playground\n\n\n\n\n\n\nJul 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBook your space\n\n\nAn open source application for managing office desk booking\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/book-your-space/index.html",
    "href": "projects/book-your-space/index.html",
    "title": "Book your space",
    "section": "",
    "text": "demo  source code\n\n\nMotivation\nThe COVID-19 pandemic breakout has led to a significant shift in the way we work, with many employees now working remotely or in a hybrid model. This has created a need for new tools and technologies to help businesses manage their office space more effectively.\nThis application for managing desk office booking is designed to address this challenge.\n\n\nHow it works\nBuild on Google apps script platform, the application is designed to work in a Google enviroment.\nIt allows:\n\nPerson quota managing over 2 weeks of booking\nDisplay names customization\nBookings download\n\nFork the script here and adapt it on your needs."
  },
  {
    "objectID": "posts/on-bayesian-optimization/index.html",
    "href": "posts/on-bayesian-optimization/index.html",
    "title": "On Bayesian optimization",
    "section": "",
    "text": "Background"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Welcome here,\nI’m a Statistician with a degree in actuarial science.  I’m passionate about unconvering hidden patterns and extracting actionable insights from complex data.   Some of my projects are here.  Some of my notebooks.  Quick articles on interesting fields."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "",
    "section": "",
    "text": "On Casual Inference\n\n\n\n\n\n\n\npython\n\n\n\n\nUnscrambling the chicken and the egg\n\n\n\n\n\n\nFeb 3, 2024\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/tripscraper/index.html",
    "href": "projects/tripscraper/index.html",
    "title": "Tripscraper",
    "section": "",
    "text": "source code"
  },
  {
    "objectID": "projects/tripscraper/index.html#footnotes",
    "href": "projects/tripscraper/index.html#footnotes",
    "title": "Tripscraper",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJulia Silge, David Robinson; Text Mining with R; O’Reilly↩︎"
  },
  {
    "objectID": "notebooks.html",
    "href": "notebooks.html",
    "title": "",
    "section": "",
    "text": "Apriori analysis for quality assurance\n\n\n\n\n\n\n\nR\n\n\nApriori analysis\n\n\n\n\nPriori Patterns for Posterior Perfection\n\n\n\n\n\n\nAug 9, 2024\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nDividi et impera\n\n\n\n\n\n\n\npython\n\n\nclustering\n\n\n\n\nUnleashing the Power of Centroid Domination in the Realm of Clustering\n\n\n\n\n\n\nOct 24, 2023\n\n\n6 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/hr-resources/index.html",
    "href": "notebooks/hr-resources/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Background"
  },
  {
    "objectID": "notebooks/hr-resources/index.html#metrics",
    "href": "notebooks/hr-resources/index.html#metrics",
    "title": "A predictable unbalanced problem",
    "section": "Metrics",
    "text": "Metrics\n\nAccuracy"
  },
  {
    "objectID": "posts/on-casual-inference/index.html",
    "href": "posts/on-casual-inference/index.html",
    "title": "On Casual Inference",
    "section": "",
    "text": "Background"
  },
  {
    "objectID": "notebooks/apriori-defects/index.html",
    "href": "notebooks/apriori-defects/index.html",
    "title": "Apriori analysis for quality assurance",
    "section": "",
    "text": "The Apriori algorithm is a data mining technique that can help identify frequent patterns and associations among defects or issues encountered during process management activities. Instead of analyzing individual defects in isolation, the Apriori 1 algorithm allows us to uncover relationships between different types of defects or issues that often occur together.\nIn the context of process management quality assurance, the algorithm works as follows:\nBy analyzing these metrics, the Apriori algorithm can reveal meaningful associations between different types of defects or issues in the process management context. For example, it may uncover that a particular defect X and issue Y often occur together with high confidence, suggesting a potential root cause or shared underlying problem in the process.\nThese insights can be valuable for the quality assurance team in several ways:\nrules &lt;- apriori(trans,\n1                 parameter = list(supp=3/length(items_list),\n2                                  conf=0.1,\n                                  target= \"rules\"),\n                 control = list(verbose=FALSE))\n\n\n1\n\n3/length(df) means minimum 3 out of total transactions must contain an itemset for it to be considered frequent\n\n2\n\nconf=0.1 means the minimum confidence threshold is set to 0.1 or 10%. This filters out association rules where the consequent (right-hand side) occurs less than 10% of the times when the antecedent (left-hand side) occurs"
  },
  {
    "objectID": "notebooks/apriori-defects/index.html#lift-interpretation",
    "href": "notebooks/apriori-defects/index.html#lift-interpretation",
    "title": "Apriori analysis for quality assurance",
    "section": "Lift: interpretation",
    "text": "Lift: interpretation\nLet’s define the following:\n\\(X\\) = \\(D_{18}\\)\n\\(Y\\) = \\(D_7\\)\nThe probability of the defect \\(D_7\\) occurring, without considering any other factors, is 5%. In other words, in 5% of all cases, the defect \\(D_7\\) is present.\nHowever, when you look at cases where the defect \\(D_{18}\\) is present, you find that the probability of the defect \\(D_7\\) occurring is ~41.9%.\nUsing the lift formula, we can calculate the lift of the rule “\\(D_{18}\\) → \\(D_7\\)” as follows:\n\\[\\text{lift}({X} \\rightarrow {Y}) = \\frac{P({Y} | {X})}{P({Y})} = \\frac{0.419}{0.05} = 8.39\\]\nThe lift value of 8.39 indicates that when the defect \\(D_{18}\\) is present, the probability of the defect \\(D_7\\) occurring is 8.39 times higher than the probability of the defect \\(D_7\\) occurring without considering the presence of \\(D_{18}\\).\nIn other words, if the defect \\(D_{18}\\) is present, it is a strong indicator or warning sign that the defect \\(D_7\\) is likely to occur as well. This positive correlation between the two defects could be valuable for identifying root causes, implementing preventive measures, or prioritizing process improvements in the system or process where these defects are observed.\nThe lift value greater than 1 suggests that the occurrence of \\(D_{18}\\) and \\(D_7\\) together is not independent or random, but rather there is a strong positive association between the two defects. This information can be used to further investigate the relationship between \\(D_{18}\\) and \\(D_7\\) and potentially uncover underlying factors or dependencies that contribute to their co-occurrence."
  },
  {
    "objectID": "notebooks/apriori-defects/index.html#confidence-interpretation",
    "href": "notebooks/apriori-defects/index.html#confidence-interpretation",
    "title": "Apriori analysis for quality assurance",
    "section": "Confidence: interpretation",
    "text": "Confidence: interpretation\nBased on the confidence value of 0.75 for the association rule X → Y, where:\n\\(X\\) = \\(D_{18}\\)\n\\(Y\\) = \\(D_7\\)\nWe can draw the following conclusions:\n\nStrong association: A confidence value of 0.75 indicates a relatively strong association between the two variables. This means that when \\(D_{18}\\) error occurs, there is a 75% probability that the \\(D_7\\) will follow as error.\nProcess improvement opportunity: The strong association between X and Y could indicate potential issues or inefficiencies in the stakeholder notification process. It may be beneficial to review and improve the process to prevent errors."
  },
  {
    "objectID": "notebooks/apriori-defects/index.html#footnotes",
    "href": "notebooks/apriori-defects/index.html#footnotes",
    "title": "Apriori analysis for quality assurance",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nApriori wikipedia page↩︎"
  },
  {
    "objectID": "notebooks/clustering/index.html",
    "href": "notebooks/clustering/index.html",
    "title": "Dividi et impera",
    "section": "",
    "text": "Dataset"
  },
  {
    "objectID": "notebooks/clustering/index.html#distorsion",
    "href": "notebooks/clustering/index.html#distorsion",
    "title": "Dividi et impera",
    "section": "Distorsion",
    "text": "Distorsion\nThe distortion score is a metric used to evaluate the quality of a clustering model, specifically the K-Means algorithm. It measures the average squared distance between each data point and its assigned cluster centroid.\nThe formula is:\n\\[\\text{Distortion} = \\frac{1}{n} \\sum_{i=1}^{n} \\|x_i - c_j\\|^2\\]\nWhere:\n\n\\(n\\) is the total number of data points\n\\(x_i\\) is the \\(i\\)-th data point\n\\(c_j\\) is the centroid of the cluster that \\(x_i\\) belongs to\nThe sum is taken over all \\(n\\) data points\nThe \\(\\|x_i - c_j\\|^2\\) term represents the squared Euclidean distance between the data point \\(x_i\\) and its assigned cluster centroid \\(c_j\\)\nThe final result is the average of these squared distances across all data points\n\nThe goal of the K-Means algorithm is to find cluster assignments that minimize this distortion score, i.e., to group data points such that the average squared distance to their assigned centroids is as small as possible. A lower distortion score indicates better clustering quality."
  },
  {
    "objectID": "notebooks/clustering/index.html#silhouette-score",
    "href": "notebooks/clustering/index.html#silhouette-score",
    "title": "Dividi et impera",
    "section": "Silhouette score",
    "text": "Silhouette score\n\\[s(i) = \\frac{b(i) - a(i)}{max(a(i), b(i))}\\]\nwhere:\n\n\\(s(i)\\) is the Silhouette Coefficient for the \\(i\\)-th sample.\n\\(a(i)\\) is the mean distance between the \\(i\\)-th sample and all other samples in the same cluster.\n\\(b(i)\\) is the mean distance between the \\(i\\)-th sample and all other samples in the next nearest cluster.\n\nThe Silhouette Coefficient ranges from -1 to 1, where a high value indicates that the sample is well-matched to its own cluster and poorly-matched to neighboring clusters. A value of 0 generally indicates overlapping clusters, while a negative value indicates that the sample may have been assigned to the wrong cluster.\n\n\nCode\n# fitting kmeans\nkm = KMeans(n_clusters=3, init='k-means++', n_init=12, max_iter=2000,algorithm=\"elkan\")\n\n# Calculate silhouette score\nsilhouette_avg = round(silhouette_score(data_processed, km.fit_predict(data_processed)), 4)\n\n# silhouette coefficient plot\nsilhouette = SilhouetteVisualizer(km, colors='yellowbrick')\nsilhouette.fit(data_processed)\n\nplt.show()\n\n\n\n\n\n\nInterpreting it\nWhen the Silhouette Coefficient \\(s(i)\\) is negative, it means that the average distance \\(a(i)\\) of the \\(i\\)-th sample to other samples in its own cluster is greater than the average distance \\(b(i)\\) to the nearest neighboring cluster. This suggests that the \\(i\\)-th sample would be better assigned to the nearest neighboring cluster rather than its current cluster.\nIn the Silhouette Plot, clusters with bars that extend into the negative range indicate that some samples within those clusters have been poorly assigned. The wider the negative portion of the bar, the more samples in that cluster have been misclassified.\nThe presence of negative Silhouette Coefficient values is a sign that the clustering algorithm has not been able to find well-separated, dense clusters. It suggests that the number of clusters \\(K\\) may not be appropriate for the data, or that the clustering algorithm is not a good fit for the dataset. In such cases, the Silhouette Plot can be used to guide the selection of a more appropriate value of \\(K\\) or the choice of a different clustering algorithm.\n\n\nCode\n# intercluster distance plot\nicd = InterclusterDistance(km, legend_loc='lower left')\nicd.fit(data_processed)\n\nicd.show()\n\n\n\n\n\n&lt;Axes: title={'center': 'KMeans Intercluster Distance Map (via MDS)'}, xlabel='PC2', ylabel='PC1'&gt;"
  },
  {
    "objectID": "notebooks/clustering/index.html#intercluster-distance-map-the-pca",
    "href": "notebooks/clustering/index.html#intercluster-distance-map-the-pca",
    "title": "Dividi et impera",
    "section": "",
    "text": "Let’s assume we have a dataset \\(X \\in \\mathbb{R}^{n \\times d}\\) with \\(n\\) samples and \\(d\\) features, and a clustering model that has identified \\(k\\) clusters with centers \\(\\mathbf{c}_i \\in \\mathbb{R}^d, i=1,\\dots,k\\).\nThe goal of the intercluster distance map is to embed these \\(k\\) cluster centers into a 2-dimensional space \\(\\mathbf{z}_i \\in \\mathbb{R}^2, i=1,\\dots,k\\), while preserving the distances between the clusters in the original high-dimensional space.\nMathematically, this can be expressed as finding an embedding function \\(f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^2\\) such that the distances between the embedded cluster centers are as close as possible to the distances between the original cluster centers:\n\\[\\min_{f} \\sum_{i&lt;j} \\left| \\|\\mathbf{c}_i - \\mathbf{c}_j\\| - \\|\\mathbf{z}_i - \\mathbf{z}_j\\| \\right|\\]\nWhere \\(\\|\\cdot\\|\\) denotes the Euclidean norm.\nThe size of each cluster’s representation on the 2D plot is determined by a scoring metric, typically the “membership” score, which is the number of instances belonging to each cluster:\n\\[s_i = |\\{x \\in X | \\text{label}(x) = i\\}|\\]\nWhere \\(\\text{label}(x)\\) is the cluster assignment of the \\(x\\)-th sample.\nThe final intercluster distance map visualizes the 2D embedded cluster centers \\(\\{\\mathbf{z}_i\\}_{i=1}^k\\), with the size of each cluster proportional to its membership score \\(s_i\\).\nThis allows the user to gain insights into the relative positions and sizes of the identified clusters, and the relationships between them in the original high-dimensional feature space."
  },
  {
    "objectID": "notebooks/clustering/index.html#intercluster-distance-map-mds-embedding-technique",
    "href": "notebooks/clustering/index.html#intercluster-distance-map-mds-embedding-technique",
    "title": "Dividi et impera",
    "section": "",
    "text": "Let’s assume we have a dataset \\(X \\in \\mathbb{R}^{n \\times d}\\) with \\(n\\) samples and \\(d\\) features, and a clustering model that has identified \\(k\\) clusters with centers \\(\\mathbf{c}_i \\in \\mathbb{R}^d, i=1,\\dots,k\\).\nThe goal of the intercluster distance map is to embed these \\(k\\) cluster centers into a 2-dimensional space \\(\\mathbf{z}_i \\in \\mathbb{R}^2, i=1,\\dots,k\\), while preserving the distances between the clusters in the original high-dimensional space.\nMathematically, this can be expressed as finding an embedding function \\(f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^2\\) such that the distances between the embedded cluster centers are as close as possible to the distances between the original cluster centers:\n\\[\\min_{f} \\sum_{i&lt;j} \\left| \\|\\mathbf{c}_i - \\mathbf{c}_j\\| - \\|\\mathbf{z}_i - \\mathbf{z}_j\\| \\right|\\]\nWhere \\(\\|\\cdot\\|\\) denotes the Euclidean norm.\nThe size of each cluster’s representation on the 2D plot is determined by a scoring metric, typically the “membership” score, which is the number of instances belonging to each cluster:\n\\[s_i = |\\{x \\in X | \\text{label}(x) = i\\}|\\]\nWhere \\(\\text{label}(x)\\) is the cluster assignment of the \\(x\\)-th sample.\nThe final intercluster distance map visualizes the 2D embedded cluster centers \\(\\{\\mathbf{z}_i\\}_{i=1}^k\\), with the size of each cluster proportional to its membership score \\(s_i\\).\nThis allows the user to gain insights into the relative positions and sizes of the identified clusters, and the relationships between them in the original high-dimensional feature space."
  },
  {
    "objectID": "notebooks/clustering/index.html#intercluster-distance-map-via-mds",
    "href": "notebooks/clustering/index.html#intercluster-distance-map-via-mds",
    "title": "Dividi et impera",
    "section": "Intercluster distance map (via MDS)",
    "text": "Intercluster distance map (via MDS)\nLet’s assume we have a dataset \\(X \\in \\mathbb{R}^{n \\times d}\\) with \\(n\\) samples and \\(d\\) features, and a clustering model that has identified \\(k\\) clusters with centers \\(\\mathbf{c}_i \\in \\mathbb{R}^d, i=1,\\dots,k\\).\nThe goal of the intercluster distance map is to embed these \\(k\\) cluster centers into a 2-dimensional space \\(\\mathbf{z}_i \\in \\mathbb{R}^2, i=1,\\dots,k\\), while preserving the distances between the clusters in the original high-dimensional space.\nMathematically, this can be expressed as finding an embedding function \\(f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^2\\) such that the distances between the embedded cluster centers are as close as possible to the distances between the original cluster centers:\n\\[\\min_{f} \\sum_{i&lt;j} \\left| \\|\\mathbf{c}_i - \\mathbf{c}_j\\| - \\|\\mathbf{z}_i - \\mathbf{z}_j\\| \\right|\\]\nWhere \\(\\|\\cdot\\|\\) denotes the Euclidean norm.\nThe size of each cluster’s representation on the 2D plot is determined by a scoring metric, typically the “membership” score, which is the number of instances belonging to each cluster:\n\\[s_i = |\\{x \\in X | \\text{label}(x) = i\\}|\\]\nWhere \\(\\text{label}(x)\\) is the cluster assignment of the \\(x\\)-th sample.\nThe final intercluster distance map visualizes the 2D embedded cluster centers \\(\\{\\mathbf{z}_i\\}_{i=1}^k\\), with the size of each cluster proportional to its membership score \\(s_i\\).\nThis allows the user to gain insights into the relative positions and sizes of the identified clusters, and the relationships between them in the original high-dimensional feature space."
  }
]