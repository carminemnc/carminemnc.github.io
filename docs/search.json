[
  {
    "objectID": "projects/causalwiz/index.html",
    "href": "projects/causalwiz/index.html",
    "title": "Causalwiz",
    "section": "",
    "text": "Package doc  Source code\ncausalwiz is an R package for causal inference analysis. It provides functions for estimating treatment effects using various statistical methods, features built-in plotting functions an utilities designed for causal inference workflows in R.\nThe package is accompanied by a series of Italian-language articles, covering theoretical foundations of causal inference and the practical implementation of methods in causalwiz. Web-based package documentation is also available."
  },
  {
    "objectID": "projects/causalwiz/index.html#installation",
    "href": "projects/causalwiz/index.html#installation",
    "title": "Causalwiz",
    "section": "Installation",
    "text": "Installation\nYou can install the package via Github repository:\n# install.packages(\"pak\")\npak::pak(\"carminemnc/causalwiz\")"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "CausalwizMay 7, 2025\n\n\n\nTools for Causal Inference Analysis and Visualization\n\n\n\n\n\n\nTripscraperJul 10, 2023\n\n\n\nAn open source application for NLP playground\n\n\n\n\n\n\nBook your spaceNov 23, 2022\n\n\n\nAn open source application for managing office desk booking\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/book-your-space/index.html",
    "href": "projects/book-your-space/index.html",
    "title": "Book your space",
    "section": "",
    "text": "Demo  Source code\n\n\nMotivation\nThe COVID-19 pandemic breakout has led to a significant shift in the way we work, with many employees now working remotely or in a hybrid model. This has created a need for new tools and technologies to help businesses manage their office space more effectively.\nThis application for managing desk office booking is designed to address this challenge.\n\n\nHow it works\nBuild on Google apps script platform, the application is designed to work in a Google enviroment.\nIt allows:\n\nPerson quota managing over 2 weeks of booking\nDisplay names customization\nBookings download\n\nFork the script here and adapt it on your needs."
  },
  {
    "objectID": "posts/inferenza-causale-capitolo-zero/index.html",
    "href": "posts/inferenza-causale-capitolo-zero/index.html",
    "title": "Inferenza causale - capitolo zero",
    "section": "",
    "text": "Package doc"
  },
  {
    "objectID": "notebooks/eda/index.html",
    "href": "notebooks/eda/index.html",
    "title": "How to get your loan",
    "section": "",
    "text": "Dataset"
  },
  {
    "objectID": "notebooks/eda/index.html#loan-approval-landscape",
    "href": "notebooks/eda/index.html#loan-approval-landscape",
    "title": "How to get your loan",
    "section": "Loan approval landscape",
    "text": "Loan approval landscape\n\n\nCode\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 4))\n\n# binary ratio plot\nleo.binary_ratio_plot(data=df, column_name='loan_approved', target_zero_name=\"Not approved\", target_one_name=\"Approved\", ax=ax1)\n\n# let's see distribution of risk score by approval status\nsns.histplot(data=df, x='risk_score', hue='loan_approved', multiple=\"layer\", alpha=0.6, ax=ax2)\nax2.set_xlabel('Risk score')                     \n \nax2.set_ylabel('Frequency')\nax2.legend(labels=['Approved', 'Not approved'],loc='upper right')\n\nleo.insights_box(fig,\n                 fontsize=12,\n                 position='bottom',\n                 x=0,\n                 y=-0.02,\n                 text=\n                 \"\"\"\n                 As expected data is heavily unbalanced revealing a disparity in loan approvals. \n                 Only 23.90% are accepted, versus 76.10% rejections.\n                 \n                 Approved loans predominantly cluster between 30 and 45 points, peaking around 40.\n                 In contrast, rejected loans dominate beyond 50 points, with the highest frequency between 50 and 60.\n                 This marked separation underscores how a low risk score is crucial for loan approval, \n                 suggesting that 45-50 points might represent the critical threshold for acceptance.\n                 \"\"\")                                                                                                                                                                                                   \nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/eda/index.html#income-first-line-of-defense",
    "href": "notebooks/eda/index.html#income-first-line-of-defense",
    "title": "How to get your loan",
    "section": "Income: first line of defense",
    "text": "Income: first line of defense\n\n\nCode\nfig, (ax1, ax2, ax3) = leo.create_layout( [(0,0,1,1), (0,1,1,1), (1,0,1,2)], figsize=(10,6))\n\n# let's see the annual income of who get the loan approved\nsns.kdeplot(data=df, x='annual_income', hue='loan_approved', fill=True, alpha=0.5, ax=ax1, legend=False)\nax1.set_title('Annual income by approval status')\nax1.set_xlabel('Annual income')\nax1.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:,.0f}K'))\nax1.legend(labels=['Approved', 'Not approved'], loc='upper right')\n\n# how the income affect the loan amount requested?\nsns.scatterplot(data=df, x='annual_income', y='loan_amount', hue='loan_approved',sizes=(20, 200), alpha=0.6, ax=ax2)\nax2.set_title('Annual income vs Loan amount')\nax2.set_xlabel('Annual income')\nax2.set_ylabel('Loan amount')\nax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:,.0f}K'))\nax2.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:,.0f}K'))\nhandles, _ = ax2.get_legend_handles_labels()\nax2.legend(handles=handles, labels=['Not approved', 'Approved'], loc='upper right')\n\n# how monthly income change with different risk score levels\nsns.violinplot(data=df, x='risk_level', y='monthly_income', ax=ax3, legend=False)\nsns.swarmplot(data=df.head(1000), x='risk_level', y='monthly_income', color='black', size=2, ax=ax3, legend=False)\nax3.set_ylabel('Monthly income')\nax3.set_xlabel('Risk score')\nax3.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:,.0f}K'))\n\nleo.insights_box(fig,\n                 fontsize=12,\n                 position='top',\n                 x=0,\n                 y=1.43,\n                 text=\n                 \"\"\"\n                 Most applications come from individuals earning $0-\\$100K, increasing for for incomes above \\$100K.\n                 This suggests that a higher income does not necessarily guarantee loan approval.\n                 \n                 Approved loans are mainly concentrated in the $100K-\\$300K income range, with amounts rarely exceeding \\$50K. \n                 Interestingly, for higher incomes (&gt;\\$300K), approved loan amounts tend to remain conservative. \n                 Regarding rejected loans (blue dots), they show greater variability in requested amounts, \n                 with some applications reaching \\$175K, especially in lower income brackets (&lt;\\$100K).\n                 This suggests that loan requests disproportionate to income are more likely to be rejected.\n\n                 The income distribution widens towards lower brackets as risk score increases, \n                 indicating an inverse correlation between income and risk.\n                 \"\"\")  \n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/eda/index.html#who-gets-loans",
    "href": "notebooks/eda/index.html#who-gets-loans",
    "title": "How to get your loan",
    "section": "Who gets loans?",
    "text": "Who gets loans?\nThe analysis of demographic variables reveals a clear hierarchy of factors influencing loan approval. Education emerges as the most decisive factor, with a significant gap between approval rates for higher degrees (PhD 48-63%) and lower ones (High School 16-22%). This pattern is further reinforced by age: approval rates consistently increase with age across all education levels, suggesting that life experience and financial stability are highly valued. Professional stability also plays a key role, with self-employed and employed individuals enjoying significantly higher approval rates than the unemployed. Surprisingly, marital status has a marginal impact, suggesting a more equitable approach based on individual merit rather than social factors.\n\n\nCode\napproval_matrix = pd.pivot_table(df, values='loan_approved',index='education_level',columns='age_bins',aggfunc='mean')\n\nplt.figure(figsize=(10, 5))\nsns.heatmap(approval_matrix, annot=True, fmt='.0%',cmap=ccmap,cbar=False)\nplt.title('Loan approval rate by age and education level')\nplt.xlabel('Age Group')\nplt.ylabel('')\nplt.tight_layout()\nplt.show()\n\nfig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2, 2, figsize=(10, 7))\n\nleo.stacked_bars_plot(data=df, x='employment_status', y='loan_approved', ax=ax1)\nax1.set_title('Loan approval status by employment status')\nleo.stacked_bars_plot(data=df, x='marital_status', y='loan_approved', ax=ax2)\nax2.set_title('Loan approval status by marital status')\nleo.stacked_bars_plot(data=df, x='job_tenure_bins', y='loan_approved', ax=ax3)\nax3.set_title('Loan approval status by job tenure')\nleo.stacked_bars_plot(data=df, x='experience_bins', y='loan_approved', ax=ax4)\nax4.set_title('Loan approval status by experience')\n\nax2.legend(title='Approval status', labels=['Not approved', 'Approved'], bbox_to_anchor=(1, 1), loc='upper left')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/eda/index.html#wealth-indicators",
    "href": "notebooks/eda/index.html#wealth-indicators",
    "title": "How to get your loan",
    "section": "Wealth indicators",
    "text": "Wealth indicators\n\n\nCode\nfig, (ax1, ax2, ax3) = leo.create_layout( [(0,0,1,1), (0,1,1,1), (1,0,1,2)], figsize=(10, 7))\n\n# risk score and loan approval status by net worth level\nleo.dumbbell_plot(df=df, group_col='loan_approved',category_col='net_worth_level',value_col='risk_score',ax=ax1,labels=['Not approved','Approved'])\nax1.set_title('Risk score comparison by net worth level')\nax1.set_xlabel('Risk score')\n\n# total assets and liabilities proportions by net worth level\ndf.groupby('net_worth_level')[['asset_percentage', 'liability_percentage']].mean().rename(columns={'asset_percentage': 'Assets', 'liability_percentage': 'Liabilities'}).plot(kind='area', stacked=True, ax=ax2)\nax2.set_title('Assets and liabilities composition in net worth')\nax2.set_xlabel('Net worth level')\nax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\nax2.set_xticks(range(5))\nax2.set_xticklabels(['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n\n# the asset coverage (total assets / loan amount) ratio by risk level\nax = sns.boxplot(data=df, x='risk_level', y='asset_coverage_ratio', showfliers=False, hue='loan_approved', width=0.3, ax=ax3)\nax3.set_title('Asset coverage ratio by risk level')\nax3.set_xlabel('Risk level')\nax3.set_ylabel('Asset coverage ratio')\nhandles, _ = ax3.get_legend_handles_labels()\nax3.legend(handles=handles, labels=['Not approved', 'Approved'])\n\nleo.insights_box(fig,\n                 fontsize=12,\n                 position='bottom',\n                 x=0,\n                 y=-0.02,\n                 text=\n                 \"\"\"\n                 The risk score shows a clear correlation with net worth.\n                 'Very Low' and 'Low' levels present lower risk scores (37.5-45). 'High' and 'Very High' levels show higher scores (47.5-55).\n                 \n                 Net worth composition reveals an interesting trend.\n                 As net worth increases, the proportion of assets to liabilities significantly improves.\n\n                 The asset coverage ratio plot reveals that for 'Low' risk level, we observe the widest variability (1-20x) \n                 and highest medians (around 5x) for both approved and non-approved loans, \n                 suggesting that strong asset coverage doesn't automatically guarantee approval.\n                 \"\"\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/eda/index.html#credit-history-trust-factor",
    "href": "notebooks/eda/index.html#credit-history-trust-factor",
    "title": "How to get your loan",
    "section": "Credit history: trust factor",
    "text": "Credit history: trust factor\nBankruptcy history plays a decisive role: the majority of applicants have no bankruptcy history and show risk scores concentrated in the 50-70 range, while those with bankruptcy records display a more scattered and generally riskier distribution. The ridge plot of credit history length reveals a particularly interesting pattern in risk distribution. Low-risk profiles show a distinctive peak around 30 years of credit history, suggesting that lengthy credit experience is associated with lower risk. In contrast, high-risk profiles display a pronounced peak around 15 years, with the distribution tapering off towards longer credit histories. This divergence in peaks between high and low-risk profiles suggests that credit history longevity can be a significant indicator of applicant reliability\n\n\nCode\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n\nsns.scatterplot(data=df, x='risk_score', y='credit_score',hue='loan_approved', ax=ax1, alpha=0.5)\nax1.set_title('Credit score vs Risk score')\nax1.set_xlabel('Risk score')\nhandles, _ = ax1.get_legend_handles_labels()\nax1.legend(handles=handles, labels=['Not approved', 'Approved'], loc='upper right')\n\nsns.kdeplot(data=df, x='risk_score', hue='bankruptcy_history', fill=True, alpha=0.5, ax=ax2, legend=False)\nax2.set_title('Risk score by Bankruptcy history')\nax2.set_xlabel('Risk score')\nax2.legend(labels=['With history', 'Without history'], loc='upper right')\n\nplt.tight_layout()\nplt.show()\n\nleo.ridge_plot(data=df,x_var='length_of_credit_history',group_var='risk_level',cmap=ccmap,height=1,aspect=9,fontsize_facets=8)\nplt.xlabel('Length of credit history')\nplt.suptitle('Length of credit history by risk level',y=0.90,size=10,va='baseline')\nplt.show()"
  },
  {
    "objectID": "notebooks/eda/index.html#the-approval-formula",
    "href": "notebooks/eda/index.html#the-approval-formula",
    "title": "How to get your loan",
    "section": "The approval formula",
    "text": "The approval formula\nAs highlighted in our previous analyses, we can now precisely quantify the impact of each variable on the risk score through their correlations.\n\nOn one side, we find factors that increase risk: bankruptcy history emerges as the most critical element, with a positive correlation of 0.4, followed by total debt-to-income ratio and debt-to-income ratio (0.35). Interest rates and previous defaults also contribute to increasing risk, albeit with a more moderate impact (0.2).\nOn the other side, some factors act as protective elements, reducing risk: income, both monthly and annual, shows the strongest negative correlation (-0.4), followed by net worth and total assets (-0.3). Credit score and length of credit history confirm their protective role with moderate negative correlations (-0.25). It’s particularly interesting to note how the requested loan amount has a relatively modest impact on the risk score (0.15), suggesting that banks focus more on repayment capability than on the loan size itself.\n\n\n\nCode\n# Select numeric columns only\nnumeric_cols = cldf.select_dtypes(include=['int64', 'float64']).columns.tolist()\nnumeric_cols.remove('risk_score')\n\n# Calculate correlations with risk_score\ncorrelations = cldf[numeric_cols + ['risk_score']].corr()['risk_score'].drop(['risk_score','loan_approved']).sort_values(key=abs, ascending=False)\n\n# renaming index\ncorrelations.index = [col.replace('_', ' ').capitalize() for col in correlations.index]\n\nplt.figure(figsize=(10, 4))\nbars = correlations.head(15).plot(kind='barh', color=np.where(correlations &gt; 0, '#0085a1', '#242728'),alpha=1)\nplt.axvline(x=0, color='#242728', linestyle='--', alpha=0.1)\nplt.xlabel('Correlation')\nplt.title('Correlation with risk score')\nmplcyberpunk.add_glow_effects()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/apriori-defects/index.html",
    "href": "notebooks/apriori-defects/index.html",
    "title": "Apriori analysis for quality assurance",
    "section": "",
    "text": "The Apriori algorithm is a data mining technique that can help identify frequent patterns and associations among defects or issues encountered during process management activities. Instead of analyzing individual defects in isolation, the Apriori 1 algorithm allows us to uncover relationships between different types of defects or issues that often occur together.\n\\[\n\\begin{aligned}\n\\begin{array}{c|c}\n\\hline\n\\text { Case } & \\text { Defects } \\\\\n\\hline\nCase \\ 1 & [Defect \\ 1, Defect \\ 2,Defect \\ 3] \\\\\nCase \\ 2 & [Defect \\ 1, Defect \\ 2] \\\\\nCase \\ 3 & [Defect \\ 1, Defect \\ 4] \\\\\nCase \\ 4 & [Defect \\ 1, Defect \\ 4,Defect \\ 5,Defect \\ 6] \\\\\n\\end{array}\n\\end{aligned}\n\\]\nIn the context of process management quality assurance, the algorithm works as follows:\nBy analyzing these metrics, the Apriori algorithm can reveal meaningful associations between different types of defects or issues in the process management context. For example, it may uncover that a particular defect X and issue Y often occur together with high confidence, suggesting a potential root cause or shared underlying problem in the process.\nThese insights can be valuable for the quality assurance team in several ways:\nrules &lt;- apriori(trans,\n1                 parameter = list(supp=3/length(items_list),\n2                                  conf=0.1,\n                                  target= \"rules\"),\n                 control = list(verbose=FALSE))\n\n\n1\n\n3/length(df) means minimum 3 out of total transactions must contain an itemset for it to be considered frequent\n\n2\n\nconf=0.1 means the minimum confidence threshold is set to 0.1 or 10%. This filters out association rules where the consequent (right-hand side) occurs less than 10% of the times when the antecedent (left-hand side) occurs"
  },
  {
    "objectID": "notebooks/apriori-defects/index.html#lift-interpretation",
    "href": "notebooks/apriori-defects/index.html#lift-interpretation",
    "title": "Apriori analysis for quality assurance",
    "section": "Lift: interpretation",
    "text": "Lift: interpretation\nLet’s define the following:\n\\(X\\) = \\(D_{18}\\)\n\\(Y\\) = \\(D_7\\)\nThe probability of the defect \\(D_7\\) occurring, without considering any other factors, is 5%. In other words, in 5% of all cases, the defect \\(D_7\\) is present.\nHowever, when you look at cases where the defect \\(D_{18}\\) is present, you find that the probability of the defect \\(D_7\\) occurring is ~41.9%.\nUsing the lift formula, we can calculate the lift of the rule “\\(D_{18}\\) → \\(D_7\\)” as follows:\n\\[\\text{lift}({X} \\rightarrow {Y}) = \\frac{P({Y} | {X})}{P({Y})} = \\frac{0.419}{0.05} = 8.39\\]\nThe lift value of 8.39 indicates that when the defect \\(D_{18}\\) is present, the probability of the defect \\(D_7\\) occurring is 8.39 times higher than the probability of the defect \\(D_7\\) occurring without considering the presence of \\(D_{18}\\).\nIn other words, if the defect \\(D_{18}\\) is present, it is a strong indicator or warning sign that the defect \\(D_7\\) is likely to occur as well. This positive correlation between the two defects could be valuable for identifying root causes, implementing preventive measures, or prioritizing process improvements in the system or process where these defects are observed.\nThe lift value greater than 1 suggests that the occurrence of \\(D_{18}\\) and \\(D_7\\) together is not independent or random, but rather there is a strong positive association between the two defects. This information can be used to further investigate the relationship between \\(D_{18}\\) and \\(D_7\\) and potentially uncover underlying factors or dependencies that contribute to their co-occurrence."
  },
  {
    "objectID": "notebooks/apriori-defects/index.html#confidence-interpretation",
    "href": "notebooks/apriori-defects/index.html#confidence-interpretation",
    "title": "Apriori analysis for quality assurance",
    "section": "Confidence: interpretation",
    "text": "Confidence: interpretation\nBased on the confidence value of 0.75 for the association rule X → Y, where:\n\\(X\\) = \\(D_{18}\\)\n\\(Y\\) = \\(D_7\\)\nWe can draw the following conclusions:\n\nStrong association: A confidence value of 0.75 indicates a relatively strong association between the two variables. This means that when \\(D_{18}\\) error occurs, there is a 75% probability that the \\(D_7\\) will follow as error.\nProcess improvement opportunity: The strong association between X and Y could indicate potential issues or inefficiencies in the process. It may be beneficial to review and improve the process to prevent errors."
  },
  {
    "objectID": "notebooks/apriori-defects/index.html#footnotes",
    "href": "notebooks/apriori-defects/index.html#footnotes",
    "title": "Apriori analysis for quality assurance",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nApriori wikipedia page↩︎"
  },
  {
    "objectID": "notebooks/clustering/index.html",
    "href": "notebooks/clustering/index.html",
    "title": "Dividi et impera",
    "section": "",
    "text": "Dataset"
  },
  {
    "objectID": "notebooks/clustering/index.html#distorsion",
    "href": "notebooks/clustering/index.html#distorsion",
    "title": "Dividi et impera",
    "section": "Distorsion",
    "text": "Distorsion\nThe distortion score is a metric used to evaluate the quality of a clustering model, specifically the K-Means algorithm. It measures the average squared distance between each data point and its assigned cluster centroid.\nThe formula is:\n\\[\\text{Distortion} = \\frac{1}{n} \\sum_{i=1}^{n} \\|x_i - c_j\\|^2\\]\nWhere:\n\n\\(n\\) is the total number of data points\n\\(x_i\\) is the \\(i\\)-th data point\n\\(c_j\\) is the centroid of the cluster that \\(x_i\\) belongs to\nThe sum is taken over all \\(n\\) data points\nThe \\(\\|x_i - c_j\\|^2\\) term represents the squared Euclidean distance between the data point \\(x_i\\) and its assigned cluster centroid \\(c_j\\)\nThe final result is the average of these squared distances across all data points\n\nThe goal of the K-Means algorithm is to find cluster assignments that minimize this distortion score, i.e., to group data points such that the average squared distance to their assigned centroids is as small as possible. A lower distortion score indicates better clustering quality."
  },
  {
    "objectID": "notebooks/clustering/index.html#silhouette-coefficient",
    "href": "notebooks/clustering/index.html#silhouette-coefficient",
    "title": "Dividi et impera",
    "section": "Silhouette coefficient",
    "text": "Silhouette coefficient\n\\[s(i) = \\frac{b(i) - a(i)}{max(a(i), b(i))}\\]\nwhere:\n\n\\(s(i)\\) is the Silhouette coefficient for the \\(i\\)-th sample.\n\\(a(i)\\) is the mean distance between the \\(i\\)-th sample and all other samples in the same cluster.\n\\(b(i)\\) is the mean distance between the \\(i\\)-th sample and all other samples in the next nearest cluster.\n\nThe Silhouette Coefficient ranges from -1 to 1, where a high value indicates that the sample is well-matched to its own cluster and poorly-matched to neighboring clusters. A value of 0 generally indicates overlapping clusters, while a negative value indicates that the sample may have been assigned to the wrong cluster.\n\n\nCode\n# fitting kmeans\nkm = KMeans(n_clusters=3, init='k-means++', n_init=12, max_iter=2000,algorithm=\"elkan\")\n\n# Calculate silhouette score\nsilhouette_avg = round(silhouette_score(data_processed, km.fit_predict(data_processed)), 4)\n\n# silhouette coefficient plot\nsilhouette = SilhouetteVisualizer(km, colors='yellowbrick')\nsilhouette.fit(data_processed)\n\nplt.show()\n\n\n\n\n\n\nInterpreting it\nWhen the Silhouette coefficient \\(s(i)\\) is negative, it means that the average distance \\(a(i)\\) of the \\(i\\)-th sample to other samples in its own cluster is greater than the average distance \\(b(i)\\) to the nearest neighboring cluster. This suggests that the \\(i\\)-th sample would be better assigned to the nearest neighboring cluster rather than its current cluster.\nIn the Silhouette plot, clusters with bars that extend into the negative range indicate that some samples within those clusters have been poorly assigned. The wider the negative portion of the bar, the more samples in that cluster have been misclassified.\nThe presence of negative Silhouette Coefficient values is a sign that the clustering algorithm has not been able to find well-separated, dense clusters. It suggests that the number of clusters \\(K\\) may not be appropriate for the data, or that the clustering algorithm is not a good fit for the dataset. In such cases, the Silhouette plot can be used to guide the selection of a more appropriate value of \\(K\\) or the choice of a different clustering algorithm.\n\n\nCode\n# intercluster distance plot\nicd = InterclusterDistance(km, legend_loc='lower left')\nicd.fit(data_processed)\n\nicd.show()\nplt.show()"
  },
  {
    "objectID": "notebooks/clustering/index.html#intercluster-distance-map-via-mds",
    "href": "notebooks/clustering/index.html#intercluster-distance-map-via-mds",
    "title": "Dividi et impera",
    "section": "Intercluster distance map (via MDS)",
    "text": "Intercluster distance map (via MDS)\nLet’s assume we have a dataset \\(X \\in \\mathbb{R}^{n \\times d}\\) with \\(n\\) samples and \\(d\\) features, and a clustering model that has identified \\(k\\) clusters with centers \\(\\mathbf{c}_i \\in \\mathbb{R}^d, i=1,\\dots,k\\).\nThe goal of the intercluster distance map is to embed these \\(k\\) cluster centers into a 2-dimensional space \\(\\mathbf{z}_i \\in \\mathbb{R}^2, i=1,\\dots,k\\), while preserving the distances between the clusters in the original high-dimensional space.\nMathematically, this can be expressed as finding an embedding function \\(f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^2\\) such that the distances between the embedded cluster centers are as close as possible to the distances between the original cluster centers:\n\\[\\min_{f} \\sum_{i&lt;j} \\left| \\|\\mathbf{c}_i - \\mathbf{c}_j\\| - \\|\\mathbf{z}_i - \\mathbf{z}_j\\| \\right|\\]\nWhere \\(\\|\\cdot\\|\\) denotes the Euclidean norm.\nThe size of each cluster’s representation on the 2D plot is determined by a scoring metric, typically the “membership” score, which is the number of instances belonging to each cluster:\n\\[s_i = |\\{x \\in X | \\text{label}(x) = i\\}|\\]\nWhere \\(\\text{label}(x)\\) is the cluster assignment of the \\(x\\)-th sample.\nThe final intercluster distance map visualizes the 2D embedded cluster centers \\(\\{\\mathbf{z}_i\\}_{i=1}^k\\), with the size of each cluster proportional to its membership score \\(s_i\\).\nThis allows us to gain insights into the relative positions and sizes of the identified clusters, and the relationships between them in the original high-dimensional feature space."
  },
  {
    "objectID": "notebooks.html",
    "href": "notebooks.html",
    "title": "Notebooks",
    "section": "",
    "text": "How to get your loanJun 1, 2025\n\n\n\nA data-driven journey through the factors that make or break loan applications\n\n\n\n\n\n\nApriori analysis for quality assuranceAug 9, 2024\n\n\n\nPriori Patterns for Posterior Perfection\n\n\n\n\n\n\nDividi et imperaOct 24, 2023\n\n\n\nUnleashing the Power of Centroid Domination in the Realm of Clustering\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Inferenza causale - capitolo zeroFeb 2, 2026\n\n\n\nIntroduzione al mondo causale\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/tripscraper/index.html",
    "href": "projects/tripscraper/index.html",
    "title": "Tripscraper",
    "section": "",
    "text": "Source code"
  },
  {
    "objectID": "projects/tripscraper/index.html#footnotes",
    "href": "projects/tripscraper/index.html#footnotes",
    "title": "Tripscraper",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJulia Silge, David Robinson; Text Mining with R; O’Reilly↩︎"
  }
]