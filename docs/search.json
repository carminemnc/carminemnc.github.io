[
  {
    "objectID": "projects/book-your-space/index.html",
    "href": "projects/book-your-space/index.html",
    "title": "Book your space",
    "section": "",
    "text": "clear\n demo  source code\n\n\nMotivation\nThe COVID-19 pandemic breakout has led to a significant shift in the way we work, with many employees now working remotely or in a hybrid model. This has created a need for new tools and technologies to help businesses manage their office space more effectively.\nThis application for managing desk office booking is designed to address this challenge.\n\n\nHow it works\nBuild on Google apps script platform, the application is designed to work in a Google enviroment.\nIt allows:\n\nPerson quota managing over 2 weeks of booking\nDisplay names customization\nBookings download\n\nFork the script here and adapt it on your needs."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Causalwiz\n\n\nTools for Causal Inference Analysis and Visualization\n\n\n\n\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTripscraper\n\n\nAn open source application for NLP playground\n\n\n\n\n\n\nJul 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBook your space\n\n\nAn open source application for managing office desk booking\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/causalwiz/index.html",
    "href": "projects/causalwiz/index.html",
    "title": "Causalwiz",
    "section": "",
    "text": "source code\ncausalwiz is an R package for causal inference analysis. It provides tools for estimating treatment effects using various methods including Inverse Probability Weighting (IPW) and Augmented Inverse Probability Weighting (AIPW).\nThe package is accompanied by a series of articles, written in Italian, covering theoretical foundations of causal inference and practical implementations of the methods implemented in causalwiz."
  },
  {
    "objectID": "projects/causalwiz/index.html#installation",
    "href": "projects/causalwiz/index.html#installation",
    "title": "Causalwiz",
    "section": "Installation",
    "text": "Installation\nYou can install the package via Github repository:\n# install.packages(\"pak\")\npak::pak(\"carminemnc/causalwiz\")"
  },
  {
    "objectID": "projects/causalwiz/index.html#usage",
    "href": "projects/causalwiz/index.html#usage",
    "title": "Causalwiz",
    "section": "Usage",
    "text": "Usage\nlibrary(causalwiz)\n\n# Load example data\ndata(\"welfare_small\")\n\n# Perform causal analysis\nresults &lt;- ipw_estimators(\n  data = welfare_small,\n  estimation_method = \"IPW\",\n  outcome = \"y\",\n  treatment = \"w\",\n  covariates = c(\"age\", \"polviews\", \"income\", \"educ\", \"marital\", \"sex\")\n)"
  },
  {
    "objectID": "posts/casual-inference-fchap/index.html",
    "href": "posts/casual-inference-fchap/index.html",
    "title": "Inferenza causale pt.1",
    "section": "",
    "text": "L’inferenza causale è fondamentale perché ci permette di comprendere le relazioni tra cause ed effetti nei fenomeni che osserviamo. A differenza dell’analisi della semplice associazione tra variabili, l’inferenza causale mira a stabilire relazioni causali, ovvero a identificare l’effetto di una variabile (il trattamento) sulla variabile di interesse (l’outcome), tenendo conto di tutte le possibili covariate.\nEsempi concreti di applicazione dell’inferenza causale possiamo trovarli in:\n\nEconomia: Gli economisti utilizzano l’inferenza causale per studiare l’impatto di politiche economiche, come cambiamenti nelle tasse o negli investimenti pubblici, sulla disoccupazione, i redditi e la crescita. Questo permette di valutare l’efficacia delle misure economiche.\nCriminologia: I ricercatori applicano l’inferenza causale per comprendere i fattori che influenzano la criminalità, come la relazione tra povertà, istruzione e tasso di criminalità. Ciò informa le politiche di prevenzione e contrasto della criminalità.\nMedicina: La ricerca clinica utilizza l’inferenza causale per stabilire eventuali relazioni tra l’assunzione di farmaci e l’insorgenza di effetti collaterali, al fine di migliorarne la sicurezza."
  },
  {
    "objectID": "posts/casual-inference-fchap/index.html#perchè-linferenza-causale",
    "href": "posts/casual-inference-fchap/index.html#perchè-linferenza-causale",
    "title": "Inferenza causale pt.1",
    "section": "",
    "text": "L’inferenza causale è fondamentale perché ci permette di comprendere le relazioni tra cause ed effetti nei fenomeni che osserviamo. A differenza dell’analisi della semplice associazione tra variabili, l’inferenza causale mira a stabilire relazioni causali, ovvero a identificare l’effetto di una variabile (il trattamento) sulla variabile di interesse (l’outcome), tenendo conto di tutte le possibili covariate.\nEsempi concreti di applicazione dell’inferenza causale possiamo trovarli in:\n\nEconomia: Gli economisti utilizzano l’inferenza causale per studiare l’impatto di politiche economiche, come cambiamenti nelle tasse o negli investimenti pubblici, sulla disoccupazione, i redditi e la crescita. Questo permette di valutare l’efficacia delle misure economiche.\nCriminologia: I ricercatori applicano l’inferenza causale per comprendere i fattori che influenzano la criminalità, come la relazione tra povertà, istruzione e tasso di criminalità. Ciò informa le politiche di prevenzione e contrasto della criminalità.\nMedicina: La ricerca clinica utilizza l’inferenza causale per stabilire eventuali relazioni tra l’assunzione di farmaci e l’insorgenza di effetti collaterali, al fine di migliorarne la sicurezza."
  },
  {
    "objectID": "posts/casual-inference-fchap/index.html#i-dati-che-utilizzeremo",
    "href": "posts/casual-inference-fchap/index.html#i-dati-che-utilizzeremo",
    "title": "Inferenza causale pt.1",
    "section": "I dati che utilizzeremo:",
    "text": "I dati che utilizzeremo:\n\n# causalwiz is a free package for causal inference\n# https://github.com/carminemnc/causalwiz\nlibrary(causalwiz)\nlibrary(ggplot2)\nlibrary(lmtest)\nlibrary(sandwich)\ntheme_set(ctheme(\"dark\"))\n\n\n\nCode\n# Load data\ndata(welfare_small)\ndata &lt;- welfare_small\n\n# Treatment: does the the gov't spend too much on \"welfare\" (1) or \"assistance to the poor\" (0)\ntreatment &lt;- \"w\"\n\n# Outcome: 1 for 'yes', 0 for 'no'\noutcome &lt;- \"y\"\n\n# Additional covariates\ncovariates &lt;- c(\"age\", \"polviews\", \"income\", \"educ\", \"marital\", \"sex\")\n\n\nIn questo capitolo utilizzeremo una versiona abbreviata di un dataset pubblico fornito da (Smith,2016)\nIl contesto riguarda un sondaggio in cui agli individui veniva chiesto il loro parere sulla spesa del governo americano per il welfare.\nIl trattamento, \\(W\\), consiste nella formulazione del quesito in due formulazioni diverse:\n\nAd una metà dei partecipanti il quesito veniva posto sul tema “welfare”\nAll’altra metà dei partecipanti al sondaggio il quesito veniva posto sottoforma di “assistenza ai poveri”\n\nL’outcome è una variabile binaria \\(Y_i\\) che indica se la risposta è stata positiva o meno. Oltre al trattamento e all’outcome il dataset contiene informazioni demografiche come l’età, l’orientamento politico1, il reddito, l’educazione, lo stato civile e il sesso."
  },
  {
    "objectID": "posts/casual-inference-fchap/index.html#notazione",
    "href": "posts/casual-inference-fchap/index.html#notazione",
    "title": "Inferenza causale pt.1",
    "section": "1. Notazione",
    "text": "1. Notazione\nSia \\((\\mathbf{X}_i, W_i, Y_i)\\) per l’individuo \\(i\\), dove:\n\n\\(\\mathbf{X}_i\\) è il vettore di covariate osservate per l’individuo \\(i\\)\n\\(W_i \\in \\{0, 1\\}\\) indica l’assegnazione al trattamento (0 = controllo, 1 = trattamento)\n\\(Y_i\\) è l’outcome osservato\n\nAssumiamo che i dati siano generati in modo indipendente e i.i.d. (identicamente distribuito).\nIntroduciamo i concetti di potenziali outcomes:\n\n\\(Y_i(1)\\) è l’outcome potenziale dell’individuo \\(i\\) se fosse stato assegnato al trattamento\n\\(Y_i(0)\\) è l’outcome potenziale dell’individuo \\(i\\) se fosse stato assegnato al controllo\n\nQuindi l’outcome osservato \\(Y_i\\) corrisponde a uno dei due potenziali outcomes:\n\\[ Y_i \\equiv Y_i(W_i) = \\begin{cases} Y_i(1) & \\text{se } W_i = 1 \\\\ Y_i(0) & \\text{se } W_i = 0 \\end{cases}  \\tag{1}\\]\n\nIl nostro obiettivo\nL’obiettivo è stimare l’effetto medio del trattamento (in inglese ATE, Average treatment effect ):\n\\[ \\tau = \\mathbb{E}[Y_i(1) - Y_i(0)] \\tag{2}\\]\n\nL’ATE fornisce una misura complessiva dell’impatto di un trattamento sulla popolazione target.\nStimare l’ATE è il primo passo per studiare i meccanismi attraverso cui un trattamento influenza gli outcome.\n\nLa stima dell’ATE è cruciale per valutare l’impatto di interventi, supportare le decisioni di policy, comprendere i meccanismi causali e progettare future ricerche empiriche in modo efficace. È un passaggio fondamentale nell’analisi causale.\n\nIn un contesto sperimentale…\nIn un contesto sperimentale, l’assegnazione al trattamento è indipendente dai potenziali outcomes:\n\\[ Y_i(1), Y_i(0) \\perp W_i  \\tag{3}\\]\nIn altre parole, non ci sono covariate che influenzano sia l’assegnazione al trattamento che l’outcome.\nCiò che ci aspetteremmo in un contesto sperimentale ad esempio è che la distribuzione delle covariate per i due outcomes, \\(Y_i(0)\\) e \\(Y_i(1)\\) siamo simili, come nel seguente esempio:\n\n\nCode\nggplot(data, aes(x = age + rnorm(nrow(data), sd = 0.1),\n                 y = polviews + rnorm(nrow(data), sd = 0.1))) +\n  geom_point(size = 2, alpha = 0.3) +\n  facet_wrap(\n    vars(ifelse(w, 'Control', 'Treatment'))\n  ) +\n  labs(\n    x = 'Age',\n    y = 'Political view',\n    title = 'Outcome distribution in randomized controlling trial'\n  )\n\n\n\n\n\n\n\nIn un contesto reale…\nIn un contesto reale (o osservazionale) però molto spesso l’esposizione al trattamento è direttamente dipendente dalle covariate.\nNell’esempio presentato qui è chiaramente visibile che:\n\nLa popolazione trattata(o di trattamento) ,\\(Y_i(1)\\) , risulta avere un età più alta e una visione politica più di destra/conservatrice\nMentre la popolazione non trattata (o di controllo), \\(Y_i(1)\\) , risulta essere più giovane e con una visione politica più di sinistra/liberale\n\n\n\nCode\n# in un contesto reale...\ndata_real &lt;- data\n\n# defining the group that will be dropped with some high probability\ngrp &lt;- ((data_real$w == 1) &  # if treated AND...\n        (\n            (data_real$age &gt; 45) |     # belongs an older group OR\n            (data_real$polviews &lt; 5)   # more conservative\n        )) | # OR\n        ((data_real$w == 0) &  # if untreated AND...\n        (\n            (data_real$age &lt; 45) |     # belongs a younger group OR\n            (data_real$polviews &gt; 4)   # more liberal\n        )) \n\n# Individuals in the group above have a smaller chance of being kept in the sample\nprob.keep &lt;- ifelse(grp, .15, .85)\nkeep.idx &lt;- as.logical(rbinom(n=nrow(data_real), prob=prob.keep, size = 1))\n\n# Dropping\ndata_real &lt;- data_real[keep.idx,]\n\n\n\n\nCode\nggplot(data_real,\n       aes(x = age + rnorm(nrow(data_real), sd = 0.1),\n           y = polviews + rnorm(nrow(data_real), sd = 0.1))) + \n  geom_point(size = 2, color = '#0085A1', alpha = 0.3) + \n  facet_wrap(\n    vars(ifelse(w, 'Control', 'Treatment'))\n  ) +\n  labs(title = \"Outcome distribution in a real scenario\",\n       x = \"Age\",\n       y = \"Political view\")\n\n\n\n\n\nQuesto ci permette di introdurre quelle che sono le ipotesi fondamentali da adottare nel momento in cui non siamo in un contesto sperimentale ma piuttosto in un contesto osservazionale, in cui per poter identificare l’effetto causale del trattamento abbiamo bisogno che le seguenti ipotesi siano valide:\n\n\nIpotesi di confondimento (unconfoundedness):\n- Questa ipotesi afferma che, una volta condizionato sulle covariate osservate \\(X_i\\), l’assegnazione al trattamento \\(W_i\\) è indipendente dai potenziali outcomes \\(Y_i(1)\\) e \\(Y_i(0)\\).\n- In altre parole, tutte le possibili fonti di selezione nel trattamento possono essere spiegate dalle covariate osservate.\n- Matematicamente: \\(Y_i(1), Y_i(0) \\perp W_i | X_i\\) - Questa ipotesi permette di identificare l’effetto causale del trattamento, altrimenti non osservabile.\n\n\nIpotesi di sovrapposizione (overlap):\n- Questa ipotesi afferma che per ogni possibile combinazione di valori delle covariate \\(X_i\\), esiste una probabilità positiva di essere assegnati sia al trattamento che al controllo.\n- In altre parole, non ci devono essere regioni del supporto delle covariate in cui tutti gli individui sono sempre trattati o sempre di controllo.\n- Matematicamente: \\(\\eta &lt; e(x) &lt; 1 - \\eta\\) per qualche \\(\\eta &gt; 0\\) e per tutti i \\(x\\), dove \\(e(x) = \\mathbb{P}[W_i = 1 | X_i = x]\\) è la propensione al trattamento. - Questa ipotesi garantisce che sia possibile confrontare unità trattate e di controllo con caratteristiche simili.\n\n\nL’ipotesi di confondimento assicura che non ci siano variabili non osservate che influenzano sia l’assegnazione al trattamento che l’outcome. L’ipotesi di sovrapposizione garantisce che ci siano unità di confronto appropriate per ogni individuo trattato.\nQueste ipotesi sono cruciali per l’applicabilità di metodi come l’IPW e l’AIPW, discussi nelle sezioni successive."
  },
  {
    "objectID": "posts/casual-inference-fchap/index.html#difference-in-means",
    "href": "posts/casual-inference-fchap/index.html#difference-in-means",
    "title": "Inferenza causale pt.1",
    "section": "2. Difference in means",
    "text": "2. Difference in means\nLo stimatore della differenza delle medie (difference-in-means) è un semplice stimatore non distorto dell’ATE. L’idea è calcolare la media dei outcomes nel gruppo di trattamento meno la media dei outcomes nel gruppo di controllo:\n\\[\n\\hat{\\tau}_\\text{DIFF} = \\frac{1}{n_1} \\sum_{i: W_i = 1} Y_i - \\frac{1}{n_0} \\sum_{i: W_i = 0} Y_i\n\\tag{4}\\]\ndove \\(n_w = |\\{i: W_i = w\\}|\\) è il numero di individui nel gruppo \\(w\\).\nLo stimatore della differenza delle medie può essere utilizzato solamente nel contesto sperimentale, ovvero quando l’assegnazione al trattamento è casuale e indipendente dai potenziali outcomes.\nIn questo caso, l’ipotesi di indipendenza tra assegnazione al trattamento e potenziali outcomes cioè l’ipotesi che \\(Y_i(1), Y_i(0) \\perp W_i\\) è soddisfatta.\nCiò significa che non ci sono fattori confondenti che influenzano sia l’assegnazione al trattamento che l’outcome.\nSotto questa ipotesi, la differenza tra la media degli outcomes nel gruppo di trattamento e la media degli outcomes nel gruppo di controllo fornisce una stima non distorta dell’effetto medio del trattamento (ATE).\nIn contesti osservazionali, invece, sono necessari metodi più avanzati come l’IPW e l’AIPW.\n\nStima di \\(\\tau\\) in un contesto sperimentale\nLa media nel gruppo di trattamento:\n\\[\n\\frac{1}{n_1} \\sum_{i: W_i = 1} Y_i\n\\]\n\n\nCode\nY &lt;- data[,outcome]\nW &lt;- data[,treatment]\nprint(mean(Y[W==1]))\n\n\n[1] 0.09211436\n\n\ne calcolando la media nel gruppo di controllo\n\\[\n\\frac{1}{n_0} \\sum_{i: W_i = 0} Y_i\n\\]\n\n\nCode\nprint(mean(Y[W==0]))\n\n\n[1] 0.438129\n\n\n\n\nCode\nfmla &lt;- formula(paste0(outcome, '~', treatment))\nols &lt;- lm(fmla, data=data)\ncoeftest(ols, vcov=vcovHC(ols, type='HC2'))[2,]\n\n\n     Estimate    Std. Error       t value      Pr(&gt;|t|) \n -0.346014670   0.004804239 -72.022788469   0.000000000 \n\n\nOtteniamo chiaramente che\n\\[\n\\hat{\\tau}_\\text{DIFF} = \\frac{1}{n_1} \\sum_{i: W_i = 1} Y_i - \\frac{1}{n_0} \\sum_{i: W_i = 0} Y_i = \\: \\sim -0.34\n\\]\nQuesto significa che, in media, le persone nel gruppo di trattamento hanno risposto in modo più negativo rispetto al gruppo di controllo2."
  },
  {
    "objectID": "posts/casual-inference-fchap/index.html#direct-estimator",
    "href": "posts/casual-inference-fchap/index.html#direct-estimator",
    "title": "Inferenza causale pt.1",
    "section": "3. Direct estimator",
    "text": "3. Direct estimator\nLo stimatore diretto (direct estimation) è un metodo per stimare l’effetto medio del trattamento (ATE) che può essere utilizzato in contesti osservazionali, ovvero quando l’assegnazione al trattamento non è casuale ma dipende dalle covariate osservate \\(X_i\\).\nQuesto stimatore sfrutta la seguente scomposizione:\n\\[ \\mathbb{E}[Y_i(1) - Y_i(0)] = \\mathbb{E}[\\mathbb{E}[Y_i|X_i, W_i=1]] - \\mathbb{E}[\\mathbb{E}[Y_i|X_i, W_i=0]] \\]\nLa procedura per applicare lo stimatore diretto è la seguente:\n\nStimare \\(\\mu(x, w) = \\mathbb{E}[Y_i|X_i=x, W_i=w]\\) utilizzando metodi di regressione non parametrici.\nPredire \\(\\hat{\\mu}(X_i, 1)\\) e \\(\\hat{\\mu}(X_i, 0)\\) per ogni osservazione.\nCalcolare la media delle differenze predette:\n\n\\[ \\hat{\\tau}_\\text{DM} = \\frac{1}{n} \\sum_{i=1}^n \\left[\\hat{\\mu}(X_i, 1) - \\hat{\\mu}(X_i, 0)\\right]  \\tag{5}\\]\nQuesto stimatore sfrutta la regressione per ottenere stime più accurate dell’ATE rispetto al semplice difference-in-means visto precedentemente.\nQuando usarlo: Lo stimatore diretto può essere utilizzato nel contesto osservazionale, quando l’ipotesi di confondimento (\\(Y_i(1), Y_i(0) \\perp W_i | X_i\\)) è soddisfatta. Ciò significa che tutte le variabili confondenti sono state misurate e incluse nelle covariate \\(X_i\\).\nQuando non usarlo: Lo stimatore diretto ha alcuni svantaggi:\n\nDipende fortemente dalla corretta specificazione del modello per \\(\\mu(x, w)\\). Se il modello è mal specificato, le stime saranno distorte.\nNon gode delle stesse proprietà asintotiche (come l’efficienza) degli stimatori più avanzati come l’AIPW.\n\nPertanto, quando possibile, è preferibile utilizzare metodi come l’AIPW, che sono più robusti alle ipotesi di modellazione."
  },
  {
    "objectID": "posts/casual-inference-fchap/index.html#ipw-inverse-propensity-weighted-estimator",
    "href": "posts/casual-inference-fchap/index.html#ipw-inverse-propensity-weighted-estimator",
    "title": "Inferenza causale pt.1",
    "section": "4. IPW (Inverse propensity-weighted estimator)",
    "text": "4. IPW (Inverse propensity-weighted estimator)\n\nSupponiamo..\nSupponiamo di voler stimare l’effetto medio di un certo intervento di apprendimento sui voti degli studenti, misurati su una scala da 0 a 100.\nEseguiamo due esperimenti separati in scuole di tipo A e B. Supponiamo che i voti tra gli studenti trattati:\n\nnella scuola A, siano approssimativamente distribuiti come \\(Y_i(1)|A \\sim \\mathcal{N}(60, 5^2)\\)\nnella scuola B, siano distribuiti come \\(Y_i(1)|B \\sim \\mathcal{N}(70, 5^2)\\)\n\nIl problema è che l’iscrizione al trattamento non è casuale, ma volontaria:\n\nNelle scuole di tipo A, solo il 5% degli studenti si iscrive al trattamento.\nNelle scuole di tipo B, il 40% degli studenti si iscrive al trattamento.\n\nPertanto, se prendessimo la media dei voti degli studenti trattati senza tenere conto dell’appartenenza scolastica, gli studenti della scuola \\(B\\), che hanno voti medi più alti, sarebbero sovrarappresentati e quindi la nostra stima dei voti degli studenti trattati sarebbe distorta verso l’alto.\nPer avere una stima non distorta dell’effetto medio del trattamento, bisogna tenere conto di questa differenza di probabilità di iscrizione tra le due scuole. In questo modo, si può ribilanciare il campione e calcolare una media ponderata dei voti degli studenti trattati che non risenta della diversa composizione dei due gruppi.\n\n\nL’IPW\n\\[ \\hat{\\tau}_\\text{IPW} = \\frac{1}{n} \\sum_{i=1}^n Y_i \\left[ \\frac{W_i}{\\hat{e}(X_i)} - \\frac{(1 - W_i)}{1 - \\hat{e}(X_i)} \\right]  \\tag{6}\\]\nL’estimatore inverse propensity-weighted (IPW) è un metodo per stimare l’effetto medio del trattamento (ATE) che può essere utilizzato in contesti osservazionali, ovvero quando l’assegnazione al trattamento non è casuale ma dipende dalle covariate \\(X_i\\).\nL’idea chiave dell’IPW è di utilizzare i pesi inversi della propensione al trattamento per “bilanciare” il confronto tra individui trattati e di controllo.\nLa procedura per applicare l’IPW è la seguente:\n\nStimare la propensione al trattamento (o propensity score), \\(e(X_i) = \\mathbb{P}[W_i = 1 | X_i]\\) , utilizzando un modello di regressione (preferibilmente non parametrico, ad esempio: logistica).\nCalcolare i pesi inversamente proporzionali alla propensione stimata:\n\nPer gli individui trattati: \\(\\frac{W_i}{\\hat{e}(X_i)}\\)\nPer gli individui di controllo: \\(\\frac{(1 - W_i)}{(1 - \\hat{e}(X_i))}\\)\n\nCalcolare la media ponderata dei outcomes\n\nQuando usarlo: L’IPW può essere utilizzato nel contesto osservazionale, quando sono soddisfatte le ipotesi di confondimento (\\(Y_i(1), Y_i(0) \\perp W_i | X_i\\)) e di sovrapposizione (\\(\\eta &lt; e(x) &lt; 1 - \\eta\\) per qualche \\(\\eta &gt; 0\\)).\nVantaggi dell’IPW:\n\nÈ un metodo semplice da implementare.\nÈ robusto: se il modello per la propensione è correttamente specificato, l’IPW fornisce stime non distorte dell’ATE.\n\nSvantaggi dell’IPW:\n\nQuando la propensione al trattamento è molto piccola, i pesi diventano molto grandi, rendendo lo stimatore instabile e con alta varianza.\n\n\n\nCode\nresults &lt;- ipw_estimators(\n  data = data_real,\n  estimation_method = 'IPW',\n  outcome = outcome,\n  treatment = treatment,\n  covariates = covariates,\n  model_specification = 'linear',\n  output = TRUE,\n)\n\n\nGenerated formula:\n w ~age+polviews+income+educ+marital+sex \n\nDifference-in-means estimation (benchmark):\n      Estimate     Std. Error        t value       Pr(&gt;|t|) \n -2.889323e-01   8.596003e-03  -3.361240e+01  1.663043e-233 \n\n IPW estimation:\n      Estimate      Std Error        t value       Pr(&gt;|t|) \n -3.243444e-01   1.338685e-02  -2.422859e+01  2.341609e-119 \n\n\n\nL’IPW non sfrutta appieno l’informazione contenuta nelle covariate per migliorare l’efficienza della stima.\n\nPer superare questi svantaggi, nella prossima sezione verrà introdotto un metodo più sofisticato, l’AIPW, che combina i vantaggi dell’IPW e della regressione diretta."
  },
  {
    "objectID": "posts/casual-inference-fchap/index.html#aipw-augmented-inverse-propensity-weighted-estimator",
    "href": "posts/casual-inference-fchap/index.html#aipw-augmented-inverse-propensity-weighted-estimator",
    "title": "Inferenza causale pt.1",
    "section": "5. AIPW (Augmented inverse propensity-weighted estimator)",
    "text": "5. AIPW (Augmented inverse propensity-weighted estimator)\nL’estimatore AIPW è disponibile in contesti con ipotesi di non confondimento e overlapping. La sua formula è la seguente:\n\\[\n\\begin{equation}\\begin{aligned}\\hat{\\tau}_\\mathrm{AIPW} &:= \\frac{1}{n} \\sum_{i=1}^n \\left(\\hat{\\mu}_{-i}(X_i, 1) - \\hat{\\mu}_{-i}(X_i, 0)\\right) + \\\\                        &\\quad \\frac{W_i}{\\hat{e}_{-i}(X_i)} (Y_i - \\hat{\\mu}_{-i}(X_i, 1)) \\: - \\\\                        &\\quad \\frac{1-W_i}{1-\\hat{e}_{-i}(X_i)} (Y_i - \\hat{\\mu}_{-i}(X_i, 0))\\end{aligned}\\end{equation}\n\\tag{7}\\]\nVantaggi dell’estimatore AIPW:\n\nDoppia robustezza: l’estimatore è corretto se almeno uno tra il modello di outcome \\(\\hat{\\mu}(X_i, w)\\) o il modello di propensity score \\(\\hat{e}(X_i)\\) è corretto.\nEfficienza: sotto ipotesi deboli, l’AIPW è asintoticamente efficiente, ovvero ha la minima varianza asintotica tra una classe ampia di stimatori.\nNormalità asintotica: l’AIPW è asintoticamente normale, permettendo un facile calcolo di errori standard e p-value.\n\nRispetto agli altri stimatori visti precedentemente:\n\nÈ più efficiente del difference-in-means anche in contesti sperimentali.\nÈ più robusto del direct estimator e dell’IPW quando i modelli sono mal specificati.\n\nSvantaggi:\n\nRichiede la stima di due modelli (outcome e propensity score), il che può essere impegnativo in pratica.\nQuando i propensity score sono molto vicini a 0 o 1, l’AIPW può avere prestazioni instabili.\n\nIn sintesi, l’AIPW è uno stimatore raccomandato in contesti osservazionali con ipotesi di non confondimento e overlapping, grazie alla sua doppia robustezza e efficienza asintotica. La sua implementazione può però risultare più complessa rispetto ad altri stimatori.\n\n\nCode\nresults &lt;- ipw_estimators(\n  data = data_real,\n  estimation_method = 'AIPW',\n  outcome = outcome,\n  treatment = treatment,\n  covariates = covariates,\n  model_specification = 'linear',\n  output = TRUE,\n  target.sample = \"control\"\n)\n\n\nGenerated formula:\n w ~age+polviews+income+educ+marital+sex \n\nDifference-in-means estimation (benchmark):\n      Estimate     Std. Error        t value       Pr(&gt;|t|) \n -2.889323e-01   8.596003e-03  -3.361240e+01  1.663043e-233 \n\n AIPW estimation:\n    Estimate    Std Error      t value     Pr(&gt;|t|) \n -0.31596114   0.01030717 -30.65450424   0.00000000"
  },
  {
    "objectID": "posts/casual-inference-fchap/index.html#elementi-di-diagnostica",
    "href": "posts/casual-inference-fchap/index.html#elementi-di-diagnostica",
    "title": "Inferenza causale pt.1",
    "section": "6. Elementi di diagnostica",
    "text": "6. Elementi di diagnostica\nCome abbiamo visto, in contesti osservazionali le distribuzioni delle covariate possono essere molto diverse tra individui trattati e non trattati. Tali discrepanze possono portare a stime distorte dell’ATE. Ci si aspetterebbe che ponderando verso l’alto o verso il basso le osservazioni in base ai pesi di propensione inversi, le loro medie siano simili.\nIn effetti, dovremmo attenderci anche che le medie delle funzioni di base delle covariate siano simili dopo la ponderazione. Questa proprietà è chiamata equilibrio. Un modo per verificarlo è il seguente:\nData una certa variabile \\(Z_i\\) (ad esempio, una covariata \\(X_{i1}\\), o un’interazione tra covariate \\(X_{i1}X_{i2}\\), o un polinomio nelle covariate \\(X_{i1}^2\\), ecc.), possiamo verificare la differenza media standardizzata assoluta (ASMD) di \\(Z_i\\) tra individui trattati e non trattati nei nostri dati,\n\\[\\frac{|\\bar{Z}_1 - \\bar{Z}_0|}{\\sqrt{s_1^2 + s_0^2}} \\tag{8}\\]\ndove \\(\\bar{Z}_1\\) e \\(\\bar{Z}_0\\) sono le medie campionarie di \\(Z_i\\), e \\(s_1\\) e \\(s_0\\) sono le deviazioni standard di \\(Z_i\\) per i due campioni di individui trattati e non trattati. Successivamente, possiamo verificare la stessa quantità per le loro controparti ponderate \\(Z_i W_i/\\hat{e}(X_i)\\) e \\(Z_i (1-W_i)/(1-\\hat{e}(X_i))\\).\nSe le nostre propensioni sono ben calibrate, l’ASMD per la versione ponderata dovrebbe essere vicina allo zero.\n\n\nCode\nresults &lt;- ipw_estimators(\n  data = data_real,\n  estimation_method = 'AIPW',\n  outcome = outcome,\n  treatment = treatment,\n  covariates = covariates,\n  model_specification = 'linear',\n  output = TRUE,\n  target.sample = \"control\"\n)\n\n\nGenerated formula:\n w ~age+polviews+income+educ+marital+sex \n\nDifference-in-means estimation (benchmark):\n      Estimate     Std. Error        t value       Pr(&gt;|t|) \n -2.889323e-01   8.596003e-03  -3.361240e+01  1.663043e-233 \n\n AIPW estimation:\n    Estimate    Std Error      t value     Pr(&gt;|t|) \n -0.31012160   0.01083692 -28.61713998   0.00000000 \n\n\nCode\nbal &lt;- aipw_balancer(\n  results$model_spec_matrix,\n  results$treatment_variable,\n  results$e_hat\n)\n\ncov_bal_plot(results$model_spec_matrix,\n             bal$unadjusted_cov,\n             bal$adjusted_cov)"
  },
  {
    "objectID": "posts/casual-inference-fchap/index.html#footnotes",
    "href": "posts/casual-inference-fchap/index.html#footnotes",
    "title": "Inferenza causale pt.1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLa variabile “Orientamento politico” cattura le opinioni politiche dei partecipanti su una scala ordinale con valori che vanno tipicamente da 1 (estremamente conservatore) a 7 (estremamente liberale). Valori più bassi indicano una visione politica più di destra/conservatrice, mentre valori più alti indicano una visione più di sinistra/liberale.↩︎\nPossiamo stimare l’ATE attraverso una regressione lineare del tipo:\n\\[\nY_i = Y_i(0) + W_i(Y_i(1) - Y_i(0))\n\\]↩︎"
  },
  {
    "objectID": "notebooks/eda/index.html",
    "href": "notebooks/eda/index.html",
    "title": "How to get your loan",
    "section": "",
    "text": "Dataset"
  },
  {
    "objectID": "notebooks/eda/index.html#loan-approval-landscape",
    "href": "notebooks/eda/index.html#loan-approval-landscape",
    "title": "How to get your loan",
    "section": "Loan approval landscape",
    "text": "Loan approval landscape\n\n\nCode\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 4))\n\n# binary ratio plot\nleo.binary_ratio_plot(data=df, column_name='loan_approved', target_zero_name=\"Not approved\", target_one_name=\"Approved\", ax=ax1)\n\n# let's see distribution of risk score by approval status\nsns.histplot(data=df, x='risk_score', hue='loan_approved', multiple=\"layer\", alpha=0.6, ax=ax2)\nax2.set_xlabel('Risk score')                     \n \nax2.set_ylabel('Frequency')\nax2.legend(labels=['Approved', 'Not approved'],loc='upper right')\n\nleo.insights_box(fig,\n                 fontsize=12,\n                 position='bottom',\n                 x=0,\n                 y=-0.02,\n                 text=\n                 \"\"\"\n                 As expected data is heavily unbalanced revealing a disparity in loan approvals. \n                 Only 23.90% are accepted, versus 76.10% rejections.\n                 \n                 Approved loans predominantly cluster between 30 and 45 points, peaking around 40.\n                 In contrast, rejected loans dominate beyond 50 points, with the highest frequency between 50 and 60.\n                 This marked separation underscores how a low risk score is crucial for loan approval, \n                 suggesting that 45-50 points might represent the critical threshold for acceptance.\n                 \"\"\")                                                                                                                                                                                                   \nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/eda/index.html#income-first-line-of-defense",
    "href": "notebooks/eda/index.html#income-first-line-of-defense",
    "title": "How to get your loan",
    "section": "Income: first line of defense",
    "text": "Income: first line of defense\n\n\nCode\nfig, (ax1, ax2, ax3) = leo.create_layout( [(0,0,1,1), (0,1,1,1), (1,0,1,2)], figsize=(10,6))\n\n# let's see the annual income of who get the loan approved\nsns.kdeplot(data=df, x='annual_income', hue='loan_approved', fill=True, alpha=0.5, ax=ax1, legend=False)\nax1.set_title('Annual income by approval status')\nax1.set_xlabel('Annual income')\nax1.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:,.0f}K'))\nax1.legend(labels=['Approved', 'Not approved'], loc='upper right')\n\n# how the income affect the loan amount requested?\nsns.scatterplot(data=df, x='annual_income', y='loan_amount', hue='loan_approved',sizes=(20, 200), alpha=0.6, ax=ax2)\nax2.set_title('Annual income vs Loan amount')\nax2.set_xlabel('Annual income')\nax2.set_ylabel('Loan amount')\nax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:,.0f}K'))\nax2.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:,.0f}K'))\nhandles, _ = ax2.get_legend_handles_labels()\nax2.legend(handles=handles, labels=['Not approved', 'Approved'], loc='upper right')\n\n# how monthly income change with different risk score levels\nsns.violinplot(data=df, x='risk_level', y='monthly_income', ax=ax3, legend=False)\nsns.swarmplot(data=df.head(1000), x='risk_level', y='monthly_income', color='black', size=2, ax=ax3, legend=False)\nax3.set_ylabel('Monthly income')\nax3.set_xlabel('Risk score')\nax3.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:,.0f}K'))\n\nleo.insights_box(fig,\n                 fontsize=12,\n                 position='top',\n                 x=0,\n                 y=1.43,\n                 text=\n                 \"\"\"\n                 Most applications come from individuals earning $0-\\$100K, increasing for for incomes above \\$100K.\n                 This suggests that a higher income does not necessarily guarantee loan approval.\n                 \n                 Approved loans are mainly concentrated in the $100K-\\$300K income range, with amounts rarely exceeding \\$50K. \n                 Interestingly, for higher incomes (&gt;\\$300K), approved loan amounts tend to remain conservative. \n                 Regarding rejected loans (blue dots), they show greater variability in requested amounts, \n                 with some applications reaching \\$175K, especially in lower income brackets (&lt;\\$100K).\n                 This suggests that loan requests disproportionate to income are more likely to be rejected.\n\n                 The income distribution widens towards lower brackets as risk score increases, \n                 indicating an inverse correlation between income and risk.\n                 \"\"\")  \n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/eda/index.html#who-gets-loans",
    "href": "notebooks/eda/index.html#who-gets-loans",
    "title": "How to get your loan",
    "section": "Who gets loans?",
    "text": "Who gets loans?\nThe analysis of demographic variables reveals a clear hierarchy of factors influencing loan approval. Education emerges as the most decisive factor, with a significant gap between approval rates for higher degrees (PhD 48-63%) and lower ones (High School 16-22%). This pattern is further reinforced by age: approval rates consistently increase with age across all education levels, suggesting that life experience and financial stability are highly valued. Professional stability also plays a key role, with self-employed and employed individuals enjoying significantly higher approval rates than the unemployed. Surprisingly, marital status has a marginal impact, suggesting a more equitable approach based on individual merit rather than social factors.\n\n\nCode\napproval_matrix = pd.pivot_table(df, values='loan_approved',index='education_level',columns='age_bins',aggfunc='mean')\n\nplt.figure(figsize=(10, 5))\nsns.heatmap(approval_matrix, annot=True, fmt='.0%',cmap=ccmap,cbar=False)\nplt.title('Loan approval rate by age and education level')\nplt.xlabel('Age Group')\nplt.ylabel('')\nplt.tight_layout()\nplt.show()\n\nfig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2, 2, figsize=(10, 7))\n\nleo.stacked_bars_plot(data=df, x='employment_status', y='loan_approved', ax=ax1)\nax1.set_title('Loan approval status by employment status')\nleo.stacked_bars_plot(data=df, x='marital_status', y='loan_approved', ax=ax2)\nax2.set_title('Loan approval status by marital status')\nleo.stacked_bars_plot(data=df, x='job_tenure_bins', y='loan_approved', ax=ax3)\nax3.set_title('Loan approval status by job tenure')\nleo.stacked_bars_plot(data=df, x='experience_bins', y='loan_approved', ax=ax4)\nax4.set_title('Loan approval status by experience')\n\nax2.legend(title='Approval status', labels=['Not approved', 'Approved'], bbox_to_anchor=(1, 1), loc='upper left')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/eda/index.html#wealth-indicators",
    "href": "notebooks/eda/index.html#wealth-indicators",
    "title": "How to get your loan",
    "section": "Wealth indicators",
    "text": "Wealth indicators\n\n\nCode\nfig, (ax1, ax2, ax3) = leo.create_layout( [(0,0,1,1), (0,1,1,1), (1,0,1,2)], figsize=(10, 7))\n\n# risk score and loan approval status by net worth level\nleo.dumbbell_plot(df=df, group_col='loan_approved',category_col='net_worth_level',value_col='risk_score',ax=ax1,labels=['Not approved','Approved'])\nax1.set_title('Risk score comparison by net worth level')\nax1.set_xlabel('Risk score')\n\n# total assets and liabilities proportions by net worth level\ndf.groupby('net_worth_level')[['asset_percentage', 'liability_percentage']].mean().rename(columns={'asset_percentage': 'Assets', 'liability_percentage': 'Liabilities'}).plot(kind='area', stacked=True, ax=ax2)\nax2.set_title('Assets and liabilities composition in net worth')\nax2.set_xlabel('Net worth level')\nax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\nax2.set_xticks(range(5))\nax2.set_xticklabels(['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n\n# the asset coverage (total assets / loan amount) ratio by risk level\nax = sns.boxplot(data=df, x='risk_level', y='asset_coverage_ratio', showfliers=False, hue='loan_approved', width=0.3, ax=ax3)\nax3.set_title('Asset coverage ratio by risk level')\nax3.set_xlabel('Risk level')\nax3.set_ylabel('Asset coverage ratio')\nhandles, _ = ax3.get_legend_handles_labels()\nax3.legend(handles=handles, labels=['Not approved', 'Approved'])\n\nleo.insights_box(fig,\n                 fontsize=12,\n                 position='bottom',\n                 x=0,\n                 y=-0.02,\n                 text=\n                 \"\"\"\n                 The risk score shows a clear correlation with net worth.\n                 'Very Low' and 'Low' levels present lower risk scores (37.5-45). 'High' and 'Very High' levels show higher scores (47.5-55).\n                 \n                 Net worth composition reveals an interesting trend.\n                 As net worth increases, the proportion of assets to liabilities significantly improves.\n\n                 The asset coverage ratio plot reveals that for 'Low' risk level, we observe the widest variability (1-20x) \n                 and highest medians (around 5x) for both approved and non-approved loans, \n                 suggesting that strong asset coverage doesn't automatically guarantee approval.\n                 \"\"\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/eda/index.html#credit-history-trust-factor",
    "href": "notebooks/eda/index.html#credit-history-trust-factor",
    "title": "How to get your loan",
    "section": "Credit history: trust factor",
    "text": "Credit history: trust factor\nBankruptcy history plays a decisive role: the majority of applicants have no bankruptcy history and show risk scores concentrated in the 50-70 range, while those with bankruptcy records display a more scattered and generally riskier distribution. The ridge plot of credit history length reveals a particularly interesting pattern in risk distribution. Low-risk profiles show a distinctive peak around 30 years of credit history, suggesting that lengthy credit experience is associated with lower risk. In contrast, high-risk profiles display a pronounced peak around 15 years, with the distribution tapering off towards longer credit histories. This divergence in peaks between high and low-risk profiles suggests that credit history longevity can be a significant indicator of applicant reliability\n\n\nCode\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n\nsns.scatterplot(data=df, x='risk_score', y='credit_score',hue='loan_approved', ax=ax1, alpha=0.5)\nax1.set_title('Credit score vs Risk score')\nax1.set_xlabel('Risk score')\nhandles, _ = ax1.get_legend_handles_labels()\nax1.legend(handles=handles, labels=['Not approved', 'Approved'], loc='upper right')\n\nsns.kdeplot(data=df, x='risk_score', hue='bankruptcy_history', fill=True, alpha=0.5, ax=ax2, legend=False)\nax2.set_title('Risk score by Bankruptcy history')\nax2.set_xlabel('Risk score')\nax2.legend(labels=['With history', 'Without history'], loc='upper right')\n\nplt.tight_layout()\nplt.show()\n\nleo.ridge_plot(data=df,x_var='length_of_credit_history',group_var='risk_level',cmap=ccmap,height=1,aspect=9,fontsize_facets=8)\nplt.xlabel('Length of credit history')\nplt.suptitle('Length of credit history by risk level',y=0.90,size=10,va='baseline')\nplt.show()"
  },
  {
    "objectID": "notebooks/eda/index.html#the-approval-formula",
    "href": "notebooks/eda/index.html#the-approval-formula",
    "title": "How to get your loan",
    "section": "The approval formula",
    "text": "The approval formula\nAs highlighted in our previous analyses, we can now precisely quantify the impact of each variable on the risk score through their correlations.\n\nOn one side, we find factors that increase risk: bankruptcy history emerges as the most critical element, with a positive correlation of 0.4, followed by total debt-to-income ratio and debt-to-income ratio (0.35). Interest rates and previous defaults also contribute to increasing risk, albeit with a more moderate impact (0.2).\nOn the other side, some factors act as protective elements, reducing risk: income, both monthly and annual, shows the strongest negative correlation (-0.4), followed by net worth and total assets (-0.3). Credit score and length of credit history confirm their protective role with moderate negative correlations (-0.25). It’s particularly interesting to note how the requested loan amount has a relatively modest impact on the risk score (0.15), suggesting that banks focus more on repayment capability than on the loan size itself.\n\n\n\nCode\n# Select numeric columns only\nnumeric_cols = cldf.select_dtypes(include=['int64', 'float64']).columns.tolist()\nnumeric_cols.remove('risk_score')\n\n# Calculate correlations with risk_score\ncorrelations = cldf[numeric_cols + ['risk_score']].corr()['risk_score'].drop(['risk_score','loan_approved']).sort_values(key=abs, ascending=False)\n\n# renaming index\ncorrelations.index = [col.replace('_', ' ').capitalize() for col in correlations.index]\n\nplt.figure(figsize=(10, 4))\nbars = correlations.head(15).plot(kind='barh', color=np.where(correlations &gt; 0, '#0085a1', '#242728'),alpha=1)\nplt.axvline(x=0, color='#242728', linestyle='--', alpha=0.1)\nplt.xlabel('Correlation')\nplt.title('Correlation with risk score')\nmplcyberpunk.add_glow_effects()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/apriori-defects/index.html",
    "href": "notebooks/apriori-defects/index.html",
    "title": "Apriori analysis for quality assurance",
    "section": "",
    "text": "The Apriori algorithm is a data mining technique that can help identify frequent patterns and associations among defects or issues encountered during process management activities. Instead of analyzing individual defects in isolation, the Apriori 1 algorithm allows us to uncover relationships between different types of defects or issues that often occur together.\n\\[\n\\begin{aligned}\n\\begin{array}{c|c}\n\\hline\n\\text { Case } & \\text { Defects } \\\\\n\\hline\nCase \\ 1 & [Defect \\ 1, Defect \\ 2,Defect \\ 3] \\\\\nCase \\ 2 & [Defect \\ 1, Defect \\ 2] \\\\\nCase \\ 3 & [Defect \\ 1, Defect \\ 4] \\\\\nCase \\ 4 & [Defect \\ 1, Defect \\ 4,Defect \\ 5,Defect \\ 6] \\\\\n\\end{array}\n\\end{aligned}\n\\]\nIn the context of process management quality assurance, the algorithm works as follows:\nBy analyzing these metrics, the Apriori algorithm can reveal meaningful associations between different types of defects or issues in the process management context. For example, it may uncover that a particular defect X and issue Y often occur together with high confidence, suggesting a potential root cause or shared underlying problem in the process.\nThese insights can be valuable for the quality assurance team in several ways:\nrules &lt;- apriori(trans,\n1                 parameter = list(supp=3/length(items_list),\n2                                  conf=0.1,\n                                  target= \"rules\"),\n                 control = list(verbose=FALSE))\n\n\n1\n\n3/length(df) means minimum 3 out of total transactions must contain an itemset for it to be considered frequent\n\n2\n\nconf=0.1 means the minimum confidence threshold is set to 0.1 or 10%. This filters out association rules where the consequent (right-hand side) occurs less than 10% of the times when the antecedent (left-hand side) occurs"
  },
  {
    "objectID": "notebooks/apriori-defects/index.html#lift-interpretation",
    "href": "notebooks/apriori-defects/index.html#lift-interpretation",
    "title": "Apriori analysis for quality assurance",
    "section": "Lift: interpretation",
    "text": "Lift: interpretation\nLet’s define the following:\n\\(X\\) = \\(D_{18}\\)\n\\(Y\\) = \\(D_7\\)\nThe probability of the defect \\(D_7\\) occurring, without considering any other factors, is 5%. In other words, in 5% of all cases, the defect \\(D_7\\) is present.\nHowever, when you look at cases where the defect \\(D_{18}\\) is present, you find that the probability of the defect \\(D_7\\) occurring is ~41.9%.\nUsing the lift formula, we can calculate the lift of the rule “\\(D_{18}\\) → \\(D_7\\)” as follows:\n\\[\\text{lift}({X} \\rightarrow {Y}) = \\frac{P({Y} | {X})}{P({Y})} = \\frac{0.419}{0.05} = 8.39\\]\nThe lift value of 8.39 indicates that when the defect \\(D_{18}\\) is present, the probability of the defect \\(D_7\\) occurring is 8.39 times higher than the probability of the defect \\(D_7\\) occurring without considering the presence of \\(D_{18}\\).\nIn other words, if the defect \\(D_{18}\\) is present, it is a strong indicator or warning sign that the defect \\(D_7\\) is likely to occur as well. This positive correlation between the two defects could be valuable for identifying root causes, implementing preventive measures, or prioritizing process improvements in the system or process where these defects are observed.\nThe lift value greater than 1 suggests that the occurrence of \\(D_{18}\\) and \\(D_7\\) together is not independent or random, but rather there is a strong positive association between the two defects. This information can be used to further investigate the relationship between \\(D_{18}\\) and \\(D_7\\) and potentially uncover underlying factors or dependencies that contribute to their co-occurrence."
  },
  {
    "objectID": "notebooks/apriori-defects/index.html#confidence-interpretation",
    "href": "notebooks/apriori-defects/index.html#confidence-interpretation",
    "title": "Apriori analysis for quality assurance",
    "section": "Confidence: interpretation",
    "text": "Confidence: interpretation\nBased on the confidence value of 0.75 for the association rule X → Y, where:\n\\(X\\) = \\(D_{18}\\)\n\\(Y\\) = \\(D_7\\)\nWe can draw the following conclusions:\n\nStrong association: A confidence value of 0.75 indicates a relatively strong association between the two variables. This means that when \\(D_{18}\\) error occurs, there is a 75% probability that the \\(D_7\\) will follow as error.\nProcess improvement opportunity: The strong association between X and Y could indicate potential issues or inefficiencies in the process. It may be beneficial to review and improve the process to prevent errors."
  },
  {
    "objectID": "notebooks/apriori-defects/index.html#footnotes",
    "href": "notebooks/apriori-defects/index.html#footnotes",
    "title": "Apriori analysis for quality assurance",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nApriori wikipedia page↩︎"
  },
  {
    "objectID": "notebooks/clustering/index.html",
    "href": "notebooks/clustering/index.html",
    "title": "Dividi et impera",
    "section": "",
    "text": "Dataset"
  },
  {
    "objectID": "notebooks/clustering/index.html#distorsion",
    "href": "notebooks/clustering/index.html#distorsion",
    "title": "Dividi et impera",
    "section": "Distorsion",
    "text": "Distorsion\nThe distortion score is a metric used to evaluate the quality of a clustering model, specifically the K-Means algorithm. It measures the average squared distance between each data point and its assigned cluster centroid.\nThe formula is:\n\\[\\text{Distortion} = \\frac{1}{n} \\sum_{i=1}^{n} \\|x_i - c_j\\|^2\\]\nWhere:\n\n\\(n\\) is the total number of data points\n\\(x_i\\) is the \\(i\\)-th data point\n\\(c_j\\) is the centroid of the cluster that \\(x_i\\) belongs to\nThe sum is taken over all \\(n\\) data points\nThe \\(\\|x_i - c_j\\|^2\\) term represents the squared Euclidean distance between the data point \\(x_i\\) and its assigned cluster centroid \\(c_j\\)\nThe final result is the average of these squared distances across all data points\n\nThe goal of the K-Means algorithm is to find cluster assignments that minimize this distortion score, i.e., to group data points such that the average squared distance to their assigned centroids is as small as possible. A lower distortion score indicates better clustering quality."
  },
  {
    "objectID": "notebooks/clustering/index.html#silhouette-coefficient",
    "href": "notebooks/clustering/index.html#silhouette-coefficient",
    "title": "Dividi et impera",
    "section": "Silhouette coefficient",
    "text": "Silhouette coefficient\n\\[s(i) = \\frac{b(i) - a(i)}{max(a(i), b(i))}\\]\nwhere:\n\n\\(s(i)\\) is the Silhouette coefficient for the \\(i\\)-th sample.\n\\(a(i)\\) is the mean distance between the \\(i\\)-th sample and all other samples in the same cluster.\n\\(b(i)\\) is the mean distance between the \\(i\\)-th sample and all other samples in the next nearest cluster.\n\nThe Silhouette Coefficient ranges from -1 to 1, where a high value indicates that the sample is well-matched to its own cluster and poorly-matched to neighboring clusters. A value of 0 generally indicates overlapping clusters, while a negative value indicates that the sample may have been assigned to the wrong cluster.\n\n\nCode\n# fitting kmeans\nkm = KMeans(n_clusters=3, init='k-means++', n_init=12, max_iter=2000,algorithm=\"elkan\")\n\n# Calculate silhouette score\nsilhouette_avg = round(silhouette_score(data_processed, km.fit_predict(data_processed)), 4)\n\n# silhouette coefficient plot\nsilhouette = SilhouetteVisualizer(km, colors='yellowbrick')\nsilhouette.fit(data_processed)\n\nplt.show()\n\n\n\n\n\n\nInterpreting it\nWhen the Silhouette coefficient \\(s(i)\\) is negative, it means that the average distance \\(a(i)\\) of the \\(i\\)-th sample to other samples in its own cluster is greater than the average distance \\(b(i)\\) to the nearest neighboring cluster. This suggests that the \\(i\\)-th sample would be better assigned to the nearest neighboring cluster rather than its current cluster.\nIn the Silhouette plot, clusters with bars that extend into the negative range indicate that some samples within those clusters have been poorly assigned. The wider the negative portion of the bar, the more samples in that cluster have been misclassified.\nThe presence of negative Silhouette Coefficient values is a sign that the clustering algorithm has not been able to find well-separated, dense clusters. It suggests that the number of clusters \\(K\\) may not be appropriate for the data, or that the clustering algorithm is not a good fit for the dataset. In such cases, the Silhouette plot can be used to guide the selection of a more appropriate value of \\(K\\) or the choice of a different clustering algorithm.\n\n\nCode\n# intercluster distance plot\nicd = InterclusterDistance(km, legend_loc='lower left')\nicd.fit(data_processed)\n\nicd.show()\n\n\n\n\n\n&lt;Axes: title={'center': 'KMeans Intercluster Distance Map (via MDS)'}, xlabel='PC2', ylabel='PC1'&gt;"
  },
  {
    "objectID": "notebooks/clustering/index.html#intercluster-distance-map-via-mds",
    "href": "notebooks/clustering/index.html#intercluster-distance-map-via-mds",
    "title": "Dividi et impera",
    "section": "Intercluster distance map (via MDS)",
    "text": "Intercluster distance map (via MDS)\nLet’s assume we have a dataset \\(X \\in \\mathbb{R}^{n \\times d}\\) with \\(n\\) samples and \\(d\\) features, and a clustering model that has identified \\(k\\) clusters with centers \\(\\mathbf{c}_i \\in \\mathbb{R}^d, i=1,\\dots,k\\).\nThe goal of the intercluster distance map is to embed these \\(k\\) cluster centers into a 2-dimensional space \\(\\mathbf{z}_i \\in \\mathbb{R}^2, i=1,\\dots,k\\), while preserving the distances between the clusters in the original high-dimensional space.\nMathematically, this can be expressed as finding an embedding function \\(f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^2\\) such that the distances between the embedded cluster centers are as close as possible to the distances between the original cluster centers:\n\\[\\min_{f} \\sum_{i&lt;j} \\left| \\|\\mathbf{c}_i - \\mathbf{c}_j\\| - \\|\\mathbf{z}_i - \\mathbf{z}_j\\| \\right|\\]\nWhere \\(\\|\\cdot\\|\\) denotes the Euclidean norm.\nThe size of each cluster’s representation on the 2D plot is determined by a scoring metric, typically the “membership” score, which is the number of instances belonging to each cluster:\n\\[s_i = |\\{x \\in X | \\text{label}(x) = i\\}|\\]\nWhere \\(\\text{label}(x)\\) is the cluster assignment of the \\(x\\)-th sample.\nThe final intercluster distance map visualizes the 2D embedded cluster centers \\(\\{\\mathbf{z}_i\\}_{i=1}^k\\), with the size of each cluster proportional to its membership score \\(s_i\\).\nThis allows us to gain insights into the relative positions and sizes of the identified clusters, and the relationships between them in the original high-dimensional feature space."
  },
  {
    "objectID": "notebooks.html",
    "href": "notebooks.html",
    "title": "Notebooks",
    "section": "",
    "text": "How to get your loan\n\n\n\n\n\n\n\npython\n\n\neda\n\n\n\n\nA data-driven journey through the factors that make or break loan applications\n\n\n\n\n\n\nJun 1, 2025\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\nApriori analysis for quality assurance\n\n\n\n\n\n\n\nR\n\n\nApriori analysis\n\n\n\n\nPriori Patterns for Posterior Perfection\n\n\n\n\n\n\nAug 9, 2024\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nDividi et impera\n\n\n\n\n\n\n\npython\n\n\nclustering\n\n\n\n\nUnleashing the Power of Centroid Domination in the Realm of Clustering\n\n\n\n\n\n\nOct 24, 2023\n\n\n6 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Inferenza causale pt.1\n\n\n\n\n\n\n\ncasual inference\n\n\nR\n\n\n\n\nIl dilemma dell’uovo e della gallina\n\n\n\n\n\n\nOct 28, 2024\n\n\n15 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/tripscraper/index.html",
    "href": "projects/tripscraper/index.html",
    "title": "Tripscraper",
    "section": "",
    "text": "source code"
  },
  {
    "objectID": "projects/tripscraper/index.html#footnotes",
    "href": "projects/tripscraper/index.html#footnotes",
    "title": "Tripscraper",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJulia Silge, David Robinson; Text Mining with R; O’Reilly↩︎"
  }
]